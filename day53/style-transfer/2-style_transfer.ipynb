{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.virtualenvs/tf1.13_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/luke/.virtualenvs/tf1.13_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/luke/.virtualenvs/tf1.13_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/luke/.virtualenvs/tf1.13_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/luke/.virtualenvs/tf1.13_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/luke/.virtualenvs/tf1.13_py3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# 之前vgg模型，RGB通道的3个均值，我们输入图像时，需要减去这三个均值，这三个均值是写在vgg16 net的代码中的\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "       load parameters from pre-train models.把预训练好的模型的权重拿进来\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):  \n",
    "        # 把权重拿进来\n",
    "        self.data_dict = data_dict\n",
    "    \n",
    "    def get_conv_filter(self, name): \n",
    "        # 拿卷积层的参数，name可能是conv1_2\n",
    "        # self.data_dict[name][0]是w参数，[1]是偏置\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "    \n",
    "    def get_fc_weight(self, name):  \n",
    "        # 拿全连接层参数，和上面类似\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        # 拿偏置\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "    \n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"创建卷积层\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            #第二个参数是卷积核，这个api比layers更基础，上面第三个参数是各个维度的stide\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding='SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "    \n",
    "    \n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize = [1,2,2,1],\n",
    "                              strides = [1,2,2,1],\n",
    "                              padding = 'SAME',\n",
    "                              name = name)\n",
    "    \n",
    "    def fc_layer(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"Builds fully-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w)  # x*w\n",
    "            h = tf.nn.bias_add(h, fc_b)  # x*w+b\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "    \n",
    "    def flatten_layer(self, x, name):  # 通过展平将卷积层展平后给全连接\n",
    "        \"\"\"Builds flatten layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            # [batch_size, image_width, image_height, channel]  4维张量含义\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:  # 展开，把后3个维度相乘[image_width, image_height, channel]\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])  # -1就会变为batch_size\n",
    "            return x\n",
    "    \n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"Build VGG16 network structure.\n",
    "        Parameters:\n",
    "        - x_rgb: [1, 224, 224, 3]  # 这个设置是vgg_net的设置\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print('building model ...')\n",
    "        \n",
    "        # 原有的模型是都减去了VGG_MEAN\n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis=3)  # 切分为3份，每份只有一个通道，从轴3切割\n",
    "        x_bgr = tf.concat(\n",
    "            [b - VGG_MEAN[0],\n",
    "             g - VGG_MEAN[1],\n",
    "             r - VGG_MEAN[2]],\n",
    "            axis = 3)  # 每个通道减去均值后再次合并\n",
    "        \n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224, 224, 3]  # 做一个断言，防止后面出错\n",
    "        # 这里是第一组\n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "        # 第二组\n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        # 训练的大部分时间都会花费在下面的全连接层上\n",
    "        \n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation=None)  # fc8不加激活函数是因为最后我们要进行softmax\n",
    "        self.prob = tf.nn.softmax(self.fc8, name='prob')\n",
    "        \n",
    "        \n",
    "        print('building model finished: %4ds' % (time.time() - start_time))  # 模型构建好再次打印时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model ...\n",
      "building model finished:    4s\n"
     ]
    }
   ],
   "source": [
    "# 测试一下模型的构建时间\n",
    "vgg16_npy_path = 'vgg16.npy'\n",
    "data_dict =np.load('vgg16.npy', encoding='latin1', allow_pickle=True).item()\n",
    "vgg16_for_result = VGGNet(data_dict)\n",
    "content = tf.placeholder(tf.float32,shape=[1,224,224,3])\n",
    "vgg16_for_result.build(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf run_style_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_npy_path = 'vgg16.npy'\n",
    "content_img_path = 'gugong.jpg'  # 内容图像路径\n",
    "style_img_path = 'xingkong.jpeg'  # 风格图像路径\n",
    "\n",
    "num_steps = 100  # 训练多少步\n",
    "learning_rate = 10\n",
    "\n",
    "lambda_c = 0.1   # 内容损失的系数，如果设置为0，就是只用风格特征重建图片\n",
    "lambda_s = 500   # 风格损失系数，通过最终的打印就可以明白为什么这么大，如果为零，就是只有内容特征重建图片\n",
    "\n",
    "output_dir = './run_style_transfer'  # 输出文件夹\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "for i in zip((1,2,3),(4,5,6)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luke/.virtualenvs/tf1.13_py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "building model ...\n",
      "building model finished:    4s\n",
      "building model ...\n",
      "building model finished:    2s\n",
      "building model ...\n",
      "building model finished:    2s\n",
      "WARNING:tensorflow:From /home/luke/.virtualenvs/tf1.13_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 对图像进行初始化\n",
    "# shape表示生成张量的维度，mean是均值，stddev是标准差。truncated_normal这个函数产生正太分布，均值和标准差是自己设定的\n",
    "def initial_result(shape, mean, stddev):\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev)  # 方差、标准差\n",
    "    return tf.Variable(initial)  # 1.0要求是个变量\n",
    "\n",
    "# 读取图像数据\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img)  # (224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32)  # 转维度为(1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calulates gram matrix\n",
    "    Args:\n",
    "    - x: feaures extracted from VGG Net. shape: [1, width, height, ch]\n",
    "    \"\"\"\n",
    "    # 获取各个维度的值，b是样本数，w宽度，h高度，ch通道数\n",
    "    b, w, h, ch = x.get_shape().as_list()  # x.get_shape()返回的是一个<‘TensorShape’>的元组类型，as_list()将结果转化为list类型\n",
    "    features = tf.reshape(x, [b, h*w, ch]) # 因为w和h维度像素点特点一致，通过这种方式去求的相似性\n",
    "    # [h*w, ch] matrix -> [ch, h*w] * [h*w, ch] -> [ch, ch]  \n",
    "    # 计算任意两列的相似度，通过矩阵乘法即可，adjoint_a是把其中一个features进行转置\n",
    "    # 为了防止最终的数比较大，这里除以一个常量：矩阵维度的乘积\n",
    "    gram = tf.matmul(features, features, adjoint_a=True) / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "    \n",
    "\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)  # 随机生成一个目标图像\n",
    "\n",
    "content_val = read_img(content_img_path)  # 读内容图片值\n",
    "style_val = read_img(style_img_path)  # 读风格图片值\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])  # 这是1.0版本需要的\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])  # placeholder需要喂输入\n",
    "\n",
    "data_dict = np.load(vgg16_npy_path, encoding='latin1',allow_pickle=True).item()\n",
    "# 创建3个vggnet\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)  # 内容\n",
    "vgg_for_style.build(style)  # 风格\n",
    "vgg_for_result.build(result)  # 结果\n",
    "\n",
    "# 下面的层次也是超参数，多层效果比较好\n",
    "# 可以加其他层特征来尝试，感受不同的效果\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    # vgg_for_content.conv2_2,\n",
    "    # vgg_for_content.conv3_3,\n",
    "    # vgg_for_content.conv4_3,\n",
    "    # vgg_for_content.conv5_3\n",
    "]\n",
    "\n",
    "# 结果一定要和内容的层数保持一致\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    # vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "# 也给风格特征初始化层次\n",
    "# feature_size, [1, width, height, channel]\n",
    "style_features = [\n",
    "    # vgg_for_style.conv1_2,\n",
    "    # vgg_for_style.conv2_2,\n",
    "    # vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    # vgg_for_style.conv5_3\n",
    "]\n",
    "\n",
    "# 风格图像的gram矩阵，gram矩阵是两两通道之间的相似度\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]  # style_features是一个列表\n",
    "# 给结果图像提取特征，和风格特征图像的层次必须一致\n",
    "result_style_features = [\n",
    "    # vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "# 结果图像的gram矩阵\n",
    "result_style_gram = \\\n",
    "    [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# zip: [1, 2], [3, 4], zip([1,2], [3,4]) -> [(1, 3), (2, 4)]\n",
    "# shape: [1, width, height, channel]\n",
    "# 因为是多层的，所以需要对每一层去计算损失，加起来均方误差的损失\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])  # reduce_mean分别在1，2，3轴上计算\n",
    "\n",
    "# 风格损失是gram矩阵的损失\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])  # 只有两维是因为前面求gram矩阵时已经将宽高相乘降维了\n",
    "\n",
    "# 最终的损失是内容损失和风格损失的加权\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 14286.0840, content_loss: 60794.2695, style_loss:  16.4133\n",
      "step: 2, loss_value: 11867.1758, content_loss: 46524.6992, style_loss:  14.4294\n",
      "step: 3, loss_value: 9082.7900, content_loss: 37875.5039, style_loss:  10.5905\n",
      "step: 4, loss_value: 7390.5713, content_loss: 33117.4766, style_loss:   8.1576\n",
      "step: 5, loss_value: 6972.4214, content_loss: 30414.1855, style_loss:   7.8620\n",
      "step: 6, loss_value: 6219.8516, content_loss: 28930.2773, style_loss:   6.6536\n",
      "step: 7, loss_value: 5468.9458, content_loss: 28085.8516, style_loss:   5.3207\n",
      "step: 8, loss_value: 5163.5078, content_loss: 27752.9590, style_loss:   4.7764\n",
      "step: 9, loss_value: 4762.9897, content_loss: 27515.2109, style_loss:   4.0229\n",
      "step: 10, loss_value: 4580.9131, content_loss: 27357.0430, style_loss:   3.6904\n",
      "step: 11, loss_value: 4371.3115, content_loss: 27250.2891, style_loss:   3.2926\n",
      "step: 12, loss_value: 4282.2114, content_loss: 27105.4863, style_loss:   3.1433\n",
      "step: 13, loss_value: 4078.8037, content_loss: 26776.2637, style_loss:   2.8024\n",
      "step: 14, loss_value: 3985.2124, content_loss: 26335.3730, style_loss:   2.7034\n",
      "step: 15, loss_value: 3803.8228, content_loss: 25816.5762, style_loss:   2.4443\n",
      "step: 16, loss_value: 3700.0664, content_loss: 25168.4492, style_loss:   2.3664\n",
      "step: 17, loss_value: 3545.4053, content_loss: 24374.8730, style_loss:   2.2158\n",
      "step: 18, loss_value: 3406.9385, content_loss: 23538.7520, style_loss:   2.1061\n",
      "step: 19, loss_value: 3286.4180, content_loss: 22680.0938, style_loss:   2.0368\n",
      "step: 20, loss_value: 3137.7725, content_loss: 21762.6973, style_loss:   1.9230\n",
      "step: 21, loss_value: 3005.0867, content_loss: 20843.1992, style_loss:   1.8415\n",
      "step: 22, loss_value: 2895.5122, content_loss: 19924.0508, style_loss:   1.8062\n",
      "step: 23, loss_value: 2774.3857, content_loss: 18988.4980, style_loss:   1.7511\n",
      "step: 24, loss_value: 2643.1196, content_loss: 18112.8809, style_loss:   1.6637\n",
      "step: 25, loss_value: 2535.4084, content_loss: 17280.7227, style_loss:   1.6147\n",
      "step: 26, loss_value: 2450.6433, content_loss: 16481.4961, style_loss:   1.6050\n",
      "step: 27, loss_value: 2365.4033, content_loss: 15771.0264, style_loss:   1.5766\n",
      "step: 28, loss_value: 2318.7747, content_loss: 15071.2568, style_loss:   1.6233\n",
      "step: 29, loss_value: 2215.2729, content_loss: 14481.3955, style_loss:   1.5343\n",
      "step: 30, loss_value: 2132.3179, content_loss: 13895.2930, style_loss:   1.4856\n",
      "step: 31, loss_value: 2028.0767, content_loss: 13387.8652, style_loss:   1.3786\n",
      "step: 32, loss_value: 1972.7004, content_loss: 12901.7676, style_loss:   1.3650\n",
      "step: 33, loss_value: 1948.1616, content_loss: 12426.3867, style_loss:   1.4110\n",
      "step: 34, loss_value: 1918.2594, content_loss: 12038.7295, style_loss:   1.4288\n",
      "step: 35, loss_value: 1931.8629, content_loss: 11636.4209, style_loss:   1.5364\n",
      "step: 36, loss_value: 1785.6013, content_loss: 11325.3340, style_loss:   1.3061\n",
      "step: 37, loss_value: 1728.4707, content_loss: 11005.1826, style_loss:   1.2559\n",
      "step: 38, loss_value: 1717.1743, content_loss: 10712.7529, style_loss:   1.2918\n",
      "step: 39, loss_value: 1700.7212, content_loss: 10473.5068, style_loss:   1.3067\n",
      "step: 40, loss_value: 1669.2319, content_loss: 10199.6641, style_loss:   1.2985\n",
      "step: 41, loss_value: 1589.4255, content_loss: 9982.2988, style_loss:   1.1824\n",
      "step: 42, loss_value: 1568.0361, content_loss: 9764.0088, style_loss:   1.1833\n",
      "step: 43, loss_value: 1544.4836, content_loss: 9540.3320, style_loss:   1.1809\n",
      "step: 44, loss_value: 1540.8429, content_loss: 9360.9736, style_loss:   1.2095\n",
      "step: 45, loss_value: 1535.9138, content_loss: 9156.6826, style_loss:   1.2405\n",
      "step: 46, loss_value: 1471.3292, content_loss: 9005.5908, style_loss:   1.1415\n",
      "step: 47, loss_value: 1459.8921, content_loss: 8834.8682, style_loss:   1.1528\n",
      "step: 48, loss_value: 1398.8102, content_loss: 8695.2783, style_loss:   1.0586\n",
      "step: 49, loss_value: 1405.4377, content_loss: 8566.4023, style_loss:   1.0976\n",
      "step: 50, loss_value: 1390.9292, content_loss: 8412.7988, style_loss:   1.0993\n",
      "step: 51, loss_value: 1397.4083, content_loss: 8294.2529, style_loss:   1.1360\n",
      "step: 52, loss_value: 1458.5266, content_loss: 8153.7417, style_loss:   1.2863\n",
      "step: 53, loss_value: 1379.0435, content_loss: 8087.4858, style_loss:   1.1406\n",
      "step: 54, loss_value: 1369.9270, content_loss: 7970.4819, style_loss:   1.1458\n",
      "step: 55, loss_value: 1306.1382, content_loss: 7894.4692, style_loss:   1.0334\n",
      "step: 56, loss_value: 1343.0234, content_loss: 7842.4819, style_loss:   1.1176\n",
      "step: 57, loss_value: 1338.0195, content_loss: 7745.8481, style_loss:   1.1269\n",
      "step: 58, loss_value: 1313.8486, content_loss: 7698.6685, style_loss:   1.0880\n",
      "step: 59, loss_value: 1279.3265, content_loss: 7622.2681, style_loss:   1.0342\n",
      "step: 60, loss_value: 1236.6721, content_loss: 7566.8892, style_loss:   0.9600\n",
      "step: 61, loss_value: 1239.4071, content_loss: 7501.1670, style_loss:   0.9786\n",
      "step: 62, loss_value: 1230.1776, content_loss: 7412.9395, style_loss:   0.9778\n",
      "step: 63, loss_value: 1231.8835, content_loss: 7345.4146, style_loss:   0.9947\n",
      "step: 64, loss_value: 1245.6481, content_loss: 7241.7065, style_loss:   1.0430\n",
      "step: 65, loss_value: 1225.8778, content_loss: 7189.5723, style_loss:   1.0138\n",
      "step: 66, loss_value: 1213.3151, content_loss: 7087.2129, style_loss:   1.0092\n",
      "step: 67, loss_value: 1149.1969, content_loss: 7021.6860, style_loss:   0.8941\n",
      "step: 68, loss_value: 1158.6545, content_loss: 6954.8306, style_loss:   0.9263\n",
      "step: 69, loss_value: 1167.0806, content_loss: 6868.2432, style_loss:   0.9605\n",
      "step: 70, loss_value: 1173.4556, content_loss: 6821.4556, style_loss:   0.9826\n",
      "step: 71, loss_value: 1218.4454, content_loss: 6725.0215, style_loss:   1.0919\n",
      "step: 72, loss_value: 1161.4490, content_loss: 6686.2847, style_loss:   0.9856\n",
      "step: 73, loss_value: 1159.1693, content_loss: 6630.6465, style_loss:   0.9922\n",
      "step: 74, loss_value: 1099.4814, content_loss: 6583.9507, style_loss:   0.8822\n",
      "step: 75, loss_value: 1132.4937, content_loss: 6556.5439, style_loss:   0.9537\n",
      "step: 76, loss_value: 1123.2893, content_loss: 6503.6504, style_loss:   0.9458\n",
      "step: 77, loss_value: 1117.2660, content_loss: 6474.6543, style_loss:   0.9396\n",
      "step: 78, loss_value: 1141.0051, content_loss: 6414.4272, style_loss:   0.9991\n",
      "step: 79, loss_value: 1119.9166, content_loss: 6402.1479, style_loss:   0.9594\n",
      "step: 80, loss_value: 1128.4121, content_loss: 6345.6724, style_loss:   0.9877\n",
      "step: 81, loss_value: 1066.8059, content_loss: 6323.0327, style_loss:   0.8690\n",
      "step: 82, loss_value: 1078.9850, content_loss: 6291.0410, style_loss:   0.8998\n",
      "step: 83, loss_value: 1049.8904, content_loss: 6237.9414, style_loss:   0.8522\n",
      "step: 84, loss_value: 1051.8636, content_loss: 6196.9961, style_loss:   0.8643\n",
      "step: 85, loss_value: 1048.6284, content_loss: 6156.7007, style_loss:   0.8659\n",
      "step: 86, loss_value: 1095.9827, content_loss: 6133.0063, style_loss:   0.9654\n",
      "step: 87, loss_value: 1231.3284, content_loss: 6052.8584, style_loss:   1.2521\n",
      "step: 88, loss_value: 1158.6316, content_loss: 6065.8027, style_loss:   1.1041\n",
      "step: 89, loss_value: 1109.1656, content_loss: 6055.2222, style_loss:   1.0073\n",
      "step: 90, loss_value: 1064.2896, content_loss: 6071.3247, style_loss:   0.9143\n",
      "step: 91, loss_value: 1098.2720, content_loss: 6088.8359, style_loss:   0.9788\n",
      "step: 92, loss_value: 1075.3044, content_loss: 6081.1924, style_loss:   0.9344\n",
      "step: 93, loss_value: 1054.9592, content_loss: 6102.5474, style_loss:   0.8894\n",
      "step: 94, loss_value: 1029.5300, content_loss: 6092.5356, style_loss:   0.8406\n",
      "step: 95, loss_value: 1039.2626, content_loss: 6055.6382, style_loss:   0.8674\n",
      "step: 96, loss_value: 1012.3511, content_loss: 6023.9043, style_loss:   0.8199\n",
      "step: 97, loss_value: 1011.7292, content_loss: 5966.3813, style_loss:   0.8302\n",
      "step: 98, loss_value: 991.6561, content_loss: 5911.7900, style_loss:   0.8010\n",
      "step: 99, loss_value: 981.5590, content_loss: 5839.2729, style_loss:   0.7953\n",
      "step: 100, loss_value: 970.4709, content_loss: 5777.7212, style_loss:   0.7854\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps): # 训练步骤\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                         content: content_val,  # 输入内容图像\n",
    "                         style: style_val,  # 输入风格图像\n",
    "                     })\n",
    "        # 每训练一步打印一次\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \n",
    "              % (step+1,\n",
    "                 loss_value[0],\n",
    "                 content_loss_value[0],\n",
    "                 style_loss_value[0]))  # 每次训练打印loss，content_loss，style_loss\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step+1))  # 每一步都把结果图像存储\n",
    "        result_val = result.eval(sess)[0]  # 本身维度是(1, 224, 224, 3)，[0]就是(224, 224, 3)\n",
    "#         print(result_val)\n",
    "        result_val = np.clip(result_val, 0, 255)  # 把值拉到0到255直接，小于0的变为0，大于255变为255\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)  # fromarray可以将某个ndarray变为图像\n",
    "        img.save(result_img_path)  # 保存图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2415.37168"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 16039.7168*0.1+500*1.6228"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

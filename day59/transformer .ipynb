{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport sklearn\nimport pandas as pd\nimport os\nimport sys\nimport time\nimport tensorflow as tf\n\nfrom tensorflow import keras\n\nprint(tf.__version__)\nprint(sys.version_info)\nfor module in mpl, np, pd, sklearn, tf, keras:\n    print(module.__name__, module.__version__)","metadata":{"id":"fLxdDgRm7XED","outputId":"b7501b93-167a-44cf-a28e-39a617adeff1","execution":{"iopub.status.busy":"2022-08-06T02:47:21.634870Z","iopub.execute_input":"2022-08-06T02:47:21.638633Z","iopub.status.idle":"2022-08-06T02:47:21.673816Z","shell.execute_reply.started":"2022-08-06T02:47:21.638586Z","shell.execute_reply":"2022-08-06T02:47:21.662431Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"ZCkmJZHu7XEE","outputId":"a3679a50-de97-48c8-b2f0-e215c88409e5","execution":{"iopub.status.busy":"2022-08-06T02:47:51.776196Z","iopub.execute_input":"2022-08-06T02:47:51.776894Z","iopub.status.idle":"2022-08-06T02:47:52.809900Z","shell.execute_reply.started":"2022-08-06T02:47:51.776859Z","shell.execute_reply":"2022-08-06T02:47:52.808654Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 1. loads data\n# 2. preprocesses data -> dataset\n# 3. tools\n# 3.1 generates position embedding\n# 3.2 create mask. (a. padding, b. decoder)\n# 3.3 scaled_dot_product_attention\n# 4. builds model\n    # 4.1 MultiheadAttention\n    # 4.2 EncoderLayer\n    # 4.3 DecoderLayer\n    # 4.4 EncoderModel\n    # 4.5 DecoderModel\n    # 4.6 Transformer\n# 5. optimizer & loss\n# 6. train step -> train\n# 7. Evaluate and Visualize","metadata":{"id":"e2MiPFUZ7XEG","execution":{"iopub.status.busy":"2022-08-06T02:47:57.262134Z","iopub.execute_input":"2022-08-06T02:47:57.262779Z","iopub.status.idle":"2022-08-06T02:47:57.270005Z","shell.execute_reply.started":"2022-08-06T02:47:57.262742Z","shell.execute_reply":"2022-08-06T02:47:57.268375Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\n# 葡萄牙语到英语，基于subword的,as_supervised是有监督\nexamples, info = tfds.load('ted_hrlr_translate/pt_to_en',\n                           with_info = True,\n                           as_supervised = True)\n\ntrain_examples, val_examples = examples['train'], examples['validation']\nprint(info)  # info里是数据集的描述","metadata":{"id":"O5AEi7kt7XEH","outputId":"7efc7094-384e-4477-844a-420a39e8a055","execution":{"iopub.status.busy":"2022-08-06T02:48:03.384414Z","iopub.execute_input":"2022-08-06T02:48:03.384789Z","iopub.status.idle":"2022-08-06T02:48:36.335412Z","shell.execute_reply.started":"2022-08-06T02:48:03.384760Z","shell.execute_reply":"2022-08-06T02:48:36.334367Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 葡萄牙语中有一些特除的字符，用转义字符来打印\nfor pt, en in train_examples.take(5):\n    print(pt.numpy())\n    print(en.numpy())\n    print()\nprint(train_examples)","metadata":{"id":"kC1-mKGn7XEH","outputId":"988db8b6-5d30-43d6-c296-bc938f7e3de7","execution":{"iopub.status.busy":"2022-08-06T02:48:39.844508Z","iopub.execute_input":"2022-08-06T02:48:39.844869Z","iopub.status.idle":"2022-08-06T02:48:39.938901Z","shell.execute_reply.started":"2022-08-06T02:48:39.844838Z","shell.execute_reply":"2022-08-06T02:48:39.937166Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# 转为subword数据集\n# 2**13是8192，会在8192附近，build_from_corpus\n# 在2.5版本中已经弃用，换用maybe_build_from_corpus\nen_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (en.numpy() for pt, en in train_examples),\n    target_vocab_size = 2 ** 13)\npt_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (pt.numpy() for pt, en in train_examples),\n    target_vocab_size = 2 ** 13)","metadata":{"id":"W2gpxLu97XEI","execution":{"iopub.status.busy":"2022-08-06T02:48:43.525836Z","iopub.execute_input":"2022-08-06T02:48:43.526203Z","iopub.status.idle":"2022-08-06T02:51:33.043839Z","shell.execute_reply.started":"2022-08-06T02:48:43.526171Z","shell.execute_reply":"2022-08-06T02:51:33.042756Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# 测试一个字符串,subword里边是包含空格的\nsample_string = \"Transformer is awesome.\"\n\n# tokenized_string编码后的id列表\ntokenized_string = en_tokenizer.encode(sample_string)\nprint('Tokenized string is {}'.format(tokenized_string))  \n\n# decode会自动将一个id的列表，变为原字符串\norigin_string = en_tokenizer.decode(tokenized_string)\nprint('The original string is {}'.format(origin_string))\n\nassert origin_string == sample_string\n\nfor token in tokenized_string:\n    print('{} --> \"{}\"-->{}'.format(token, en_tokenizer.decode([token]),len(en_tokenizer.decode([token]))))","metadata":{"id":"4xqF19mW7XEJ","outputId":"a0220af5-3bfc-4fe4-8107-999996c0deb5","execution":{"iopub.status.busy":"2022-08-06T02:51:33.045833Z","iopub.execute_input":"2022-08-06T02:51:33.046193Z","iopub.status.idle":"2022-08-06T02:51:33.054063Z","shell.execute_reply.started":"2022-08-06T02:51:33.046156Z","shell.execute_reply":"2022-08-06T02:51:33.053045Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"pt_tokenizer.vocab_size  # 词典的大小并不是刚好等于8192","metadata":{"id":"vTztP_ah7XEK","outputId":"50dda8d7-b94c-42e4-f59d-0a354f458717","execution":{"iopub.status.busy":"2022-08-06T02:51:33.055624Z","iopub.execute_input":"2022-08-06T02:51:33.056275Z","iopub.status.idle":"2022-08-06T02:51:33.067816Z","shell.execute_reply.started":"2022-08-06T02:51:33.056241Z","shell.execute_reply":"2022-08-06T02:51:33.066337Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"en_tokenizer.vocab_size","metadata":{"id":"772KX8T27XEM","outputId":"173d36ad-10c6-44d0-bf59-510e7f421fb6","execution":{"iopub.status.busy":"2022-08-06T02:51:33.070785Z","iopub.execute_input":"2022-08-06T02:51:33.071202Z","iopub.status.idle":"2022-08-06T02:51:33.077818Z","shell.execute_reply.started":"2022-08-06T02:51:33.071169Z","shell.execute_reply":"2022-08-06T02:51:33.076709Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"buffer_size = 20000\nbatch_size = 64\nmax_length = 40  # 这里设置输入和输出的最大长度是40，其实也可以用文本中句子的最大长度，但是训练时间会很长\n\n# 把两段文本转为subword形式\n# [pt_tokenizer.vocab_size]和[pt_tokenizer.vocab_size + 1]分别代表一句话开始标记和结束标记，和seq2seq的<start>和<end>一致\ndef encode_to_subword(pt_sentence, en_sentence):\n    pt_sequence = [pt_tokenizer.vocab_size] \\\n    + pt_tokenizer.encode(pt_sentence.numpy()) \\\n    + [pt_tokenizer.vocab_size + 1]\n    en_sequence = [en_tokenizer.vocab_size] \\\n    + en_tokenizer.encode(en_sentence.numpy()) \\\n    + [en_tokenizer.vocab_size + 1]\n    return pt_sequence, en_sequence\n\n# 用tf的API消去大于设置的最大长度的,只要葡萄牙语和英语同时小于等于40的样本\ndef filter_by_max_length(pt, en):\n    return tf.logical_and(tf.size(pt) <= max_length,\n                          tf.size(en) <= max_length)\n# 用py_function封装一下encode_to_subword，提高运行效率\ndef tf_encode_to_subword(pt_sentence, en_sentence):\n    return tf.py_function(encode_to_subword,\n                          [pt_sentence, en_sentence],\n                          [tf.int64, tf.int64])\n# 把所有句子变为subword，subword都变为id\ntrain_dataset = train_examples.map(tf_encode_to_subword)\n\n# 这部分代码是看最大长度的，pt最大长度是222，en最大长度是203\n# max1=0\n# max2=0\n# for pi,ei in train_dataset:\n#   if len(pi)>max1:\n#     max1=len(pi)\n#   if len(ei)>max2:\n#     max2=len(ei)\n# print(max1,max2) \n\ntrain_dataset = train_dataset.filter(filter_by_max_length)  # 过滤拿pt，en同时小于40的\n# 接着做洗牌，padding，batch -1，-1代表两个维度，每个维度都在当前维度下扩展到最高的值，填充到batch内的最大长度\ntrain_dataset = train_dataset.shuffle(\n    buffer_size).padded_batch(\n    batch_size, padded_shapes=([-1], [-1]))\n# 验证集只做了过滤和padding，没有洗牌\nvalid_dataset = val_examples.map(tf_encode_to_subword)\nvalid_dataset = valid_dataset.filter(\n    filter_by_max_length).padded_batch(\n    batch_size, padded_shapes=([-1], [-1]))","metadata":{"id":"0Y9P8wcJ7XEP","execution":{"iopub.status.busy":"2022-08-06T02:51:33.079547Z","iopub.execute_input":"2022-08-06T02:51:33.080198Z","iopub.status.idle":"2022-08-06T02:51:33.215046Z","shell.execute_reply.started":"2022-08-06T02:51:33.080165Z","shell.execute_reply":"2022-08-06T02:51:33.213989Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"x = tf.constant([1, 2, 3, 4])\ny = tf.constant([1, 2, 3, 4])\ntf.math.logical_and(tf.size(x)<=4,tf.size(y)<=4)","metadata":{"id":"voe3rLhD7XEP","outputId":"9e3d5f76-5dec-424c-8cb8-d7b525075c10","execution":{"iopub.status.busy":"2022-08-06T02:51:33.216487Z","iopub.execute_input":"2022-08-06T02:51:33.216873Z","iopub.status.idle":"2022-08-06T02:51:33.237712Z","shell.execute_reply.started":"2022-08-06T02:51:33.216835Z","shell.execute_reply":"2022-08-06T02:51:33.236676Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for pt_batch, en_batch in train_dataset.take(5):\n    print(pt_batch.shape, en_batch.shape)  # 填充到batch内的最大长度","metadata":{"id":"YjIqGhM87XEQ","outputId":"2fc222e5-c4fc-42ac-f146-29dac843818d","execution":{"iopub.status.busy":"2022-08-06T02:51:33.239416Z","iopub.execute_input":"2022-08-06T02:51:33.239751Z","iopub.status.idle":"2022-08-06T02:51:58.172662Z","shell.execute_reply.started":"2022-08-06T02:51:33.239718Z","shell.execute_reply":"2022-08-06T02:51:58.170888Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for pt_batch, en_batch in train_dataset.take(1):\n    print(pt_batch[0], en_batch[0])","metadata":{"id":"7LmTSxMW7XEQ","outputId":"978fc4e4-f5d1-4c0c-bc92-f00af3a9ca96","execution":{"iopub.status.busy":"2022-08-06T02:51:58.175392Z","iopub.execute_input":"2022-08-06T02:51:58.175750Z","iopub.status.idle":"2022-08-06T02:52:22.581877Z","shell.execute_reply.started":"2022-08-06T02:51:58.175720Z","shell.execute_reply":"2022-08-06T02:52:22.580970Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"np.arange(50)[:, np.newaxis]  # 冒号代表所有值所需要填充的轴","metadata":{"id":"e-pM0PAm7XER","outputId":"41b9c2c6-ca9c-4d8d-ebfe-5d2b047c386c","execution":{"iopub.status.busy":"2022-08-06T02:52:22.583738Z","iopub.execute_input":"2022-08-06T02:52:22.584741Z","iopub.status.idle":"2022-08-06T02:52:22.592094Z","shell.execute_reply.started":"2022-08-06T02:52:22.584701Z","shell.execute_reply":"2022-08-06T02:52:22.591080Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"np.arange(512)[np.newaxis, :]","metadata":{"id":"qgNH_O667XER","outputId":"e8f3ca59-36c3-4f93-a090-d278784916b0","execution":{"iopub.status.busy":"2022-08-06T02:52:22.595719Z","iopub.execute_input":"2022-08-06T02:52:22.596497Z","iopub.status.idle":"2022-08-06T02:52:22.604961Z","shell.execute_reply.started":"2022-08-06T02:52:22.596461Z","shell.execute_reply":"2022-08-06T02:52:22.603849Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# 位置编码","metadata":{"id":"WA5aKJ9slwzQ"}},{"cell_type":"code","source":"# 一些工具函数\n# PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n# PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n\n# pos 和i都是矩阵\n# pos.shape: [sentence_length, 1]\n# i.shape  : [1, d_model]\n# result.shape: [sentence_length, d_model]\ndef get_angles(pos, i, d_model):\n    angle_rates = 1 / np.power(10000,\n                               (2 * (i // 2)) / np.float32(d_model))  # 公式里0和1时2i相等，2，3时2i相等，所以写成2*(i//2)\n    return pos * angle_rates\n\n# 计算位置信息\ndef get_position_embedding(sentence_length, d_model):\n    # sentence_length和d_model都扩展为矩阵\n#     print(np.arange(sentence_length)[:, np.newaxis])\n#     print(np.arange(d_model)[np.newaxis, :])\n    # pos是0到49，就是词的位置，i是从0到511，总计512，和dim相等\n    angle_rads = get_angles(np.arange(sentence_length)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],\n                            d_model)\n#     print(angle_rads.shape)\n    # sines.shape: [sentence_length, d_model / 2]\n    # cosines.shape: [sentence_length, d_model / 2]\n#     print(angle_rads[:, 0::2].shape)\n#     print(angle_rads[:, 1::2].shape)\n    sines = np.sin(angle_rads[:, 0::2])  # 偶\n    cosines = np.cos(angle_rads[:, 1::2])  # 奇\n    \n    # 把sines和cosines进行拼接\n    # position_embedding.shape: [sentence_length, d_model]\n    position_embedding = np.concatenate([sines, cosines], axis = -1)\n    # 进行维度扩展，把[sentence_length, d_model]，变为[1, sentence_length, d_model]\n    position_embedding = position_embedding[np.newaxis, :]\n    # 变为float32类型\n    return tf.cast(position_embedding, dtype=tf.float32)\n\nposition_embedding = get_position_embedding(50, 512)\nprint(position_embedding.shape)","metadata":{"id":"CNxdgywX7XES","outputId":"d5ca96d7-ade4-4009-e719-6e67bf492d4d","execution":{"iopub.status.busy":"2022-08-06T03:09:14.168495Z","iopub.execute_input":"2022-08-06T03:09:14.168890Z","iopub.status.idle":"2022-08-06T03:09:14.185411Z","shell.execute_reply.started":"2022-08-06T03:09:14.168857Z","shell.execute_reply":"2022-08-06T03:09:14.184117Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# 这个图和原理中展示的横纵坐标是颠倒的\ndef plot_position_embedding(position_embedding):\n    plt.pcolormesh(position_embedding[0], cmap = 'RdBu')\n    plt.xlabel('Depth')\n    plt.xlim((0, 512))\n    plt.ylabel('Position')\n    plt.colorbar()\n    plt.show()\n    \nplot_position_embedding(position_embedding)","metadata":{"id":"kTDlQE7b7XES","outputId":"2e92dda1-f266-42b9-a13a-513a44645125","execution":{"iopub.status.busy":"2022-08-06T03:57:32.662367Z","iopub.execute_input":"2022-08-06T03:57:32.662731Z","iopub.status.idle":"2022-08-06T03:57:32.948365Z","shell.execute_reply.started":"2022-08-06T03:57:32.662700Z","shell.execute_reply":"2022-08-06T03:57:32.947447Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# 生成mask","metadata":{"id":"vWI_i3sDoiS_"}},{"cell_type":"code","source":"# 如何生成mask\n# 1. padding mask, 2. look ahead\n\n# batch_data.shape: [batch_size, seq_len]\ndef create_padding_mask(batch_data):\n    padding_mask = tf.cast(tf.math.equal(batch_data, 0), tf.float32)\n    # [batch_size, 1, 1, seq_len]\n    return padding_mask[:, tf.newaxis, tf.newaxis, :]\n# 设置3x5矩阵\nx = tf.constant([[7, 6, 0, 0, 0], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\ncreate_padding_mask(x)  # 是零的得到的都是1，其他的都是零","metadata":{"id":"SL35FjwW7XES","outputId":"2485feb1-17e6-438a-967e-978cf2f03dee","execution":{"iopub.status.busy":"2022-08-06T04:00:21.161355Z","iopub.execute_input":"2022-08-06T04:00:21.162331Z","iopub.status.idle":"2022-08-06T04:00:21.179122Z","shell.execute_reply.started":"2022-08-06T04:00:21.162273Z","shell.execute_reply":"2022-08-06T04:00:21.177936Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# 生成下三角矩阵\ntf.linalg.band_part(tf.ones((3, 3)), -1, 0)","metadata":{"id":"xivzYi5xqcXZ","outputId":"361989af-8a49-4343-cf38-c99dfea94a48","execution":{"iopub.status.busy":"2022-08-06T04:00:29.912986Z","iopub.execute_input":"2022-08-06T04:00:29.913532Z","iopub.status.idle":"2022-08-06T04:00:29.927600Z","shell.execute_reply.started":"2022-08-06T04:00:29.913498Z","shell.execute_reply":"2022-08-06T04:00:29.926369Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# attention_weights.shape: [3,3]\n# 第一个位置代表第一个单词和自己的attention，第二位置是第二个单词和第一个单词的attention\n# 看不到后面的词刚好是下三角，使用库函数来实现\n# [[1, 0, 0],\n#  [4, 5, 0],\n#  [7, 8, 9]]\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)  # 上三角\n    return mask # (seq_len, seq_len)\n\n# 前面看不到后面的padding，矩阵下面全部为0\n# 在mask里，应该被忽略的我们会设成1，应该被保留的会设成0，\n# 而如果mask相应位置上为1，那么我们就给对应的logits \n# 加上一个超级小的负数， -1000000000， 这样，\n# 对应的logits也就变成了一个超级小的数。然后在计算softmax的时候，\n# 一个超级小的数的指数会无限接近与0。也就是它对应的attention的权重就是0了\n# 下面可以看到\ncreate_look_ahead_mask(3)","metadata":{"id":"p1zImUKT7XET","outputId":"d12a2ae5-bc3e-4eaa-e7a1-6401d3d9a101","execution":{"iopub.status.busy":"2022-08-06T04:00:32.714210Z","iopub.execute_input":"2022-08-06T04:00:32.714598Z","iopub.status.idle":"2022-08-06T04:00:32.727508Z","shell.execute_reply.started":"2022-08-06T04:00:32.714567Z","shell.execute_reply":"2022-08-06T04:00:32.726221Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"np.exp(-1e9)","metadata":{"id":"uHXagoA57XET","outputId":"2541ee16-46ee-4ffb-976f-a0ca7ab0f227","execution":{"iopub.status.busy":"2022-08-06T04:00:43.753173Z","iopub.execute_input":"2022-08-06T04:00:43.754196Z","iopub.status.idle":"2022-08-06T04:00:43.763272Z","shell.execute_reply.started":"2022-08-06T04:00:43.754149Z","shell.execute_reply":"2022-08-06T04:00:43.761536Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# 缩放点积注意力","metadata":{"id":"VzItzKsPqqy6"}},{"cell_type":"code","source":"# 参考原理文档，q是query，k，v代表k和value，q和k做完矩阵乘法后，做mask\n# 缩放点积注意力，也叫自注意力\ndef scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"\n    Args:\n    - q: shape == (..., seq_len_q, depth)\n    - k: shape == (..., seq_len_k, depth)\n    - v: shape == (..., seq_len_v, depth_v)\n    - seq_len_k == seq_len_v  这两个是相等的!!!\n    - mask: shape == (..., seq_len_q, seq_len_k)\n    Returns:\n    - output: weighted sum\n    - attention_weights: weights of attention\n    \"\"\"\n    # 计算attentions时，我们只用了后两维在计算\n    # transpose_b代表第二个矩阵是否做转置\n    # matmul_qk.shape: (..., seq_len_q, seq_len_k)\n    matmul_qk = tf.matmul(q, k, transpose_b = True)\n    \n    # 获得dk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    # 然后根据文档中的公式除以dk，维度为  (..., seq_len_q, seq_len_k)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n    \n    # 如果mask不是空的话，给scaled_attention_logits加一个mask（缩放）\n    if mask is not None:\n        # 使得在softmax后值趋近于0\n        scaled_attention_logits += (mask * -1e9)\n    \n    # attention_weights.shape: (..., seq_len_q, seq_len_k)\n    attention_weights = tf.nn.softmax(\n        scaled_attention_logits, axis = -1)\n    \n    # 根据原理图，v和attention_weights进行矩阵乘法\n    # output.shape: (..., seq_len_q, depth_v)\n    output = tf.matmul(attention_weights, v)  # 就是这里要求seq_len_k == seq_len_v\n    \n    return output, attention_weights\n\n# 调用上面的函数，去验证\ndef print_scaled_dot_product_attention(q, k, v):\n    temp_out, temp_att = scaled_dot_product_attention(q, k, v, None)\n    print(\"Attention weights are:\")\n    print(temp_att)\n    print(\"Output is:\")\n    print(temp_out)","metadata":{"id":"9PWrvUlb7XEU","execution":{"iopub.status.busy":"2022-08-06T04:00:53.479112Z","iopub.execute_input":"2022-08-06T04:00:53.484985Z","iopub.status.idle":"2022-08-06T04:00:53.506294Z","shell.execute_reply.started":"2022-08-06T04:00:53.484939Z","shell.execute_reply":"2022-08-06T04:00:53.505409Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# 定义一个测试的Q，K，V\ntemp_k = tf.constant([[10, 0, 0],\n                      [0, 10, 0],\n                      [0, 0, 10],\n                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n\ntemp_v = tf.constant([[1, 0],\n                      [10, 0],\n                      [100, 5],\n                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n\ntemp_q1 = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n\n# 可以把这句注释，它的作用是做四舍五入，让结果清爽\nnp.set_printoptions(suppress=True)\n\nprint_scaled_dot_product_attention(temp_q1, temp_k, temp_v)","metadata":{"id":"6v6NKsoI7XEU","outputId":"56f66a6f-9f63-49e1-e036-8c60c9df8a98","execution":{"iopub.status.busy":"2022-08-06T04:00:56.366111Z","iopub.execute_input":"2022-08-06T04:00:56.366486Z","iopub.status.idle":"2022-08-06T04:00:57.109287Z","shell.execute_reply.started":"2022-08-06T04:00:56.366454Z","shell.execute_reply":"2022-08-06T04:00:57.107882Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"temp_q2 = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n#0.  0.  0.5 0.5 会和temp_v去做平均，因此得到的是550,5.5\nprint_scaled_dot_product_attention(temp_q2, temp_k, temp_v)","metadata":{"id":"vY9PoyE77XEV","outputId":"595e688c-dcd5-4f98-f4d7-3ee9bc9c1719","execution":{"iopub.status.busy":"2022-08-06T04:01:00.644869Z","iopub.execute_input":"2022-08-06T04:01:00.645225Z","iopub.status.idle":"2022-08-06T04:01:00.654959Z","shell.execute_reply.started":"2022-08-06T04:01:00.645194Z","shell.execute_reply":"2022-08-06T04:01:00.653799Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"temp_q3 = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\nprint_scaled_dot_product_attention(temp_q3, temp_k, temp_v)","metadata":{"id":"t3HfLTFR7XEV","outputId":"1e66fc3c-aaa6-4d1c-dc54-3cd20d5a7b82","execution":{"iopub.status.busy":"2022-08-06T04:01:02.924087Z","iopub.execute_input":"2022-08-06T04:01:02.924787Z","iopub.status.idle":"2022-08-06T04:01:02.934732Z","shell.execute_reply.started":"2022-08-06T04:01:02.924750Z","shell.execute_reply":"2022-08-06T04:01:02.933557Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# 拼起来再来测试\ntemp_q4 = tf.constant([[0, 10, 0],\n                       [0, 0, 10],\n                       [10, 10, 0],\n                       [10,0,10]], dtype=tf.float32)  # (3, 3)\nprint_scaled_dot_product_attention(temp_q4, temp_k, temp_v)","metadata":{"id":"_xXhQ_bO7XEV","outputId":"fc8cb50d-b69d-4912-a281-a47f1505c46d","execution":{"iopub.status.busy":"2022-08-06T04:01:05.800218Z","iopub.execute_input":"2022-08-06T04:01:05.801239Z","iopub.status.idle":"2022-08-06T04:01:05.812410Z","shell.execute_reply.started":"2022-08-06T04:01:05.801201Z","shell.execute_reply":"2022-08-06T04:01:05.811341Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"temp_k.numpy().T","metadata":{"id":"6xQh422t5Afp","outputId":"b43522fa-4e0b-4087-fb84-2c556a645a56","execution":{"iopub.status.busy":"2022-08-06T04:01:16.928650Z","iopub.execute_input":"2022-08-06T04:01:16.929019Z","iopub.status.idle":"2022-08-06T04:01:16.935323Z","shell.execute_reply.started":"2022-08-06T04:01:16.928986Z","shell.execute_reply":"2022-08-06T04:01:16.934332Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# 多头注意力","metadata":{"id":"EsBMznl9vmG7"}},{"cell_type":"code","source":"# 多头注意力的实现\nclass MultiHeadAttention(keras.layers.Layer):\n    \"\"\"\n    理论上:\n    x -> Wq0 -> q0\n    x -> Wk0 -> k0\n    x -> Wv0 -> v0\n    \n    实战中:把三个概念区分开\n    q -> Wq0 -> q0\n    k -> Wk0 -> k0\n    v -> Wv0 -> v0\n    \n    实战中技巧：q乘以W得到一个大的Q，然后分割为多个小q，拿每一个小q去做缩放点积\n    q -> Wq -> Q -> split -> q0, q1, q2...\n    \"\"\"\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        assert self.d_model % self.num_heads == 0\n        \n        # 这里对应的大Q变小q怎么变，层次\n        self.depth = self.d_model // self.num_heads\n        #神经元个数是512\n        self.WQ = keras.layers.Dense(self.d_model)\n        self.WK = keras.layers.Dense(self.d_model)\n        self.WV = keras.layers.Dense(self.d_model)\n        # 这里是拼接，拼接的输出是512\n        self.dense = keras.layers.Dense(self.d_model)\n    \n    def split_heads(self, x, batch_size):\n        # x.shape: (batch_size, seq_len, d_model)\n        # d_model = num_heads * depth\n        # 把x变为下面维度，用reshape\n        # x -> (batch_size, num_heads, seq_len, depth)\n        \n        x = tf.reshape(x,\n                       (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])  # 轴滚动，这里做轴滚动是因为缩放点积的要求\n    \n    def call(self, q, k, v, mask):\n        batch_size = tf.shape(q)[0]\n        # 经过Q K V变化\n#         print(q.shape)\n        q = self.WQ(q)  # q.shape: (batch_size, seq_len_q, d_model)\n        k = self.WK(k)  # k.shape: (batch_size, seq_len_k, d_model)\n        v = self.WV(v)  # v.shape: (batch_size, seq_len_v, d_model)\n#         print('-'*50)\n#         print(q.shape)\n        # q.shape: (batch_size, num_heads, seq_len_q, depth)\n        q = self.split_heads(q, batch_size)\n        # k.shape: (batch_size, num_heads, seq_len_k, depth)\n        k = self.split_heads(k, batch_size)\n        # v.shape: (batch_size, num_heads, seq_len_v, depth)\n        v = self.split_heads(v, batch_size)\n        \n        # 开始做缩放点积，得到的多头的信息存在在num_heads，depth上\n        # scaled_attention_outputs.shape: (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention_outputs, attention_weights = \\\n        scaled_dot_product_attention(q, k, v, mask)\n        \n        # 因此这里做一下转置，让num_heads，depth在后面\n        # scaled_attention_outputs.shape: (batch_size, seq_len_q, num_heads, depth)\n        scaled_attention_outputs = tf.transpose(\n            scaled_attention_outputs, perm = [0, 2, 1, 3])\n        \n        # 对注意力进行合并\n        # concat_attention.shape: (batch_size, seq_len_q, d_model)\n        concat_attention = tf.reshape(scaled_attention_outputs,\n                                      (batch_size, -1, self.d_model))\n        \n        # 多头注意力计算完毕\n        # output.shape : (batch_size, seq_len_q, d_model)\n        output = self.dense(concat_attention)\n        \n        return output, attention_weights\n    \ntemp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n# 创建一份虚拟数据\ny = tf.random.uniform((1, 40, 256))  # (batch_size, seq_len_q, dim)\n# 开始计算，把y既当q，又当k，v\noutput, attn = temp_mha(y, y, y, mask = None)\nprint(output.shape)  # 输出的尺寸，和x的尺寸一致\nprint(attn.shape)   # 注意力的尺寸","metadata":{"id":"Ng-rkyTg7XEV","outputId":"69b38572-aaae-44ca-d633-491e0f9acc01","execution":{"iopub.status.busy":"2022-08-06T04:01:19.199008Z","iopub.execute_input":"2022-08-06T04:01:19.199373Z","iopub.status.idle":"2022-08-06T04:01:19.263103Z","shell.execute_reply.started":"2022-08-06T04:01:19.199342Z","shell.execute_reply":"2022-08-06T04:01:19.262141Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# 前馈网络","metadata":{"id":"kMeH-lt-_5VV"}},{"cell_type":"code","source":"# 定义我们的feed_forward_network，d_model节点数\ndef feed_forward_network(d_model, dff):\n    # dff: dim of feed forward network.\n    return keras.Sequential([\n        keras.layers.Dense(dff, activation='relu'),\n        keras.layers.Dense(d_model)\n    ])\n\nsample_ffn = feed_forward_network(512, 2048)\n# 给一个输入测试\nsample_ffn(tf.random.uniform((64, 50, 512))).shape","metadata":{"id":"ej7ELB-V7XEW","outputId":"d627a38e-4cee-4325-ce8b-14a8f648baa7","execution":{"iopub.status.busy":"2022-08-06T04:01:23.004341Z","iopub.execute_input":"2022-08-06T04:01:23.005375Z","iopub.status.idle":"2022-08-06T04:01:23.043503Z","shell.execute_reply.started":"2022-08-06T04:01:23.005316Z","shell.execute_reply":"2022-08-06T04:01:23.042218Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Encoder 层","metadata":{}},{"cell_type":"code","source":"# 自定义EncoderLayer\nclass EncoderLayer(keras.layers.Layer):\n    \"\"\"\n    x -> self attention -> add & normalize & dropout\n      -> feed_forward -> add & normalize & dropout\n    原理对应文档Add & Normalize 标题下的图\n    \"\"\"\n    # d_model 给self attention和feed_forward_network，num_heads给self_attention用的\n    # dff给feed_forward_network，rate是做dropout的\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = feed_forward_network(d_model, dff)\n        \n        self.layer_norm1 = keras.layers.LayerNormalization(  # 层归一化\n            epsilon = 1e-6)\n        # epsilon 将epsilon加上方差以避免 除零问题\n        self.layer_norm2 = keras.layers.LayerNormalization(\n            epsilon = 1e-6)\n        # 下面两个层次用了做dropout，每次有10%的几率被drop掉\n        self.dropout1 = keras.layers.Dropout(rate)\n        self.dropout2 = keras.layers.Dropout(rate)\n    \n    def call(self, x, training, encoder_padding_mask):\n        # x.shape          : (batch_size, seq_len, dim=d_model)\n        # attn_output.shape: (batch_size, seq_len, d_model)\n        # out1.shape       : (batch_size, seq_len, d_model)\n        # x作为q，k，v  原理对应文档Add & Normalize 标题下的图\n        attn_output, _ = self.mha(x, x, x, encoder_padding_mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        # dim=d_model 两个必须相等，这样x才可以和attn_output做加法\n        out1 = self.layer_norm1(x + attn_output)\n        \n        # ffn_output.shape: (batch_size, seq_len, d_model)\n        # out2.shape      : (batch_size, seq_len, d_model)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layer_norm2(out1 + ffn_output)\n        \n        return out2\n# 来测试，结果和我们最初的输入维度一致，相当于做了两次残差连接\nsample_encoder_layer = EncoderLayer(512, 8, 2048)\nsample_input = tf.random.uniform((64, 50, 512))\nsample_output = sample_encoder_layer(sample_input, False, None)\nprint(sample_output.shape)","metadata":{"id":"MXUliuig7XEW","outputId":"9596ef6f-fce5-4dbf-c710-d5d13c2e39ba","execution":{"iopub.status.busy":"2022-08-06T04:01:29.095391Z","iopub.execute_input":"2022-08-06T04:01:29.095912Z","iopub.status.idle":"2022-08-06T04:01:29.165228Z","shell.execute_reply.started":"2022-08-06T04:01:29.095878Z","shell.execute_reply":"2022-08-06T04:01:29.164190Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"data = tf.constant(np.arange(10).reshape(5, 2) * 10, dtype=tf.float32)\nprint(data)","metadata":{"id":"_pzhLZWr7XEW","outputId":"539fb865-0830-44e7-928e-161f50d1f735","execution":{"iopub.status.busy":"2022-08-06T04:01:32.269849Z","iopub.execute_input":"2022-08-06T04:01:32.270875Z","iopub.status.idle":"2022-08-06T04:01:32.279696Z","shell.execute_reply.started":"2022-08-06T04:01:32.270829Z","shell.execute_reply":"2022-08-06T04:01:32.278690Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"layer = tf.keras.layers.BatchNormalization()\noutput = layer(data)\nprint(output)","metadata":{"id":"4lk6LTbe7XEW","outputId":"0d21a602-cb6e-489f-d557-df233a1f6ae5","execution":{"iopub.status.busy":"2022-08-06T04:01:34.219021Z","iopub.execute_input":"2022-08-06T04:01:34.220150Z","iopub.status.idle":"2022-08-06T04:01:34.234369Z","shell.execute_reply.started":"2022-08-06T04:01:34.220104Z","shell.execute_reply":"2022-08-06T04:01:34.232894Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"(0.9-1.65)/np.sqrt(0.44)","metadata":{"id":"ZOx5aAUu7XEX","outputId":"8c27641f-3ef6-4d71-b3f0-af90c7704b8d","execution":{"iopub.status.busy":"2022-08-06T04:01:36.434289Z","iopub.execute_input":"2022-08-06T04:01:36.435040Z","iopub.status.idle":"2022-08-06T04:01:36.441676Z","shell.execute_reply.started":"2022-08-06T04:01:36.435008Z","shell.execute_reply":"2022-08-06T04:01:36.440719Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"np.sqrt(np.sum(np.square(data.numpy()-45))/10)","metadata":{"id":"Wh1_zVRE7XEX","outputId":"3aad20ac-bdb6-458f-8f3b-8ba6e7526ff4","execution":{"iopub.status.busy":"2022-08-06T04:01:49.140416Z","iopub.execute_input":"2022-08-06T04:01:49.141074Z","iopub.status.idle":"2022-08-06T04:01:49.147983Z","shell.execute_reply.started":"2022-08-06T04:01:49.141037Z","shell.execute_reply":"2022-08-06T04:01:49.146834Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"np.sqrt(np.sum(np.square(data.numpy()[:,0]-40))/5)","metadata":{"id":"ABFdAgNu7XEX","outputId":"c1530321-30f5-4a4a-b758-ea892a8a8149","execution":{"iopub.status.busy":"2022-08-06T04:01:49.172204Z","iopub.execute_input":"2022-08-06T04:01:49.172923Z","iopub.status.idle":"2022-08-06T04:01:49.180826Z","shell.execute_reply.started":"2022-08-06T04:01:49.172888Z","shell.execute_reply":"2022-08-06T04:01:49.179626Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"sample_encoder_layer.variables","metadata":{"id":"yFmov10o7XEX","outputId":"7c69f9be-60b0-428a-bc36-343e32e26b61","execution":{"iopub.status.busy":"2022-08-06T04:01:49.182658Z","iopub.execute_input":"2022-08-06T04:01:49.184477Z","iopub.status.idle":"2022-08-06T04:01:49.470272Z","shell.execute_reply.started":"2022-08-06T04:01:49.184434Z","shell.execute_reply":"2022-08-06T04:01:49.469234Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"sample_encoder_layer.trainable_variables","metadata":{"id":"zATQINC97XEX","outputId":"cd14360d-a78e-4729-dfcd-64dc2460e86e","execution":{"iopub.status.busy":"2022-08-06T04:01:49.472808Z","iopub.execute_input":"2022-08-06T04:01:49.473488Z","iopub.status.idle":"2022-08-06T04:01:49.523096Z","shell.execute_reply.started":"2022-08-06T04:01:49.473450Z","shell.execute_reply":"2022-08-06T04:01:49.522224Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# Decoder 层","metadata":{"id":"U-7OU_lvDrEQ"}},{"cell_type":"code","source":"class DecoderLayer(keras.layers.Layer):\n    \"\"\"\n    x -> self attention -> add & normalize & dropout -> out1\n    out1 , encoding_outputs -> attention -> add & normalize & dropout -> out2\n    out2 -> ffn -> add & normalize & dropout -> out3\n    \"\"\"\n    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n        super(DecoderLayer, self).__init__()\n        \n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n        \n        self.ffn = feed_forward_network(d_model, dff)\n        \n        # 因为有两个attention，还有一个feed_forward_network，所以有3个LayerNormalization和3个dropout\n        self.layer_norm1 = keras.layers.LayerNormalization(\n            epsilon = 1e-6)\n        self.layer_norm2 = keras.layers.LayerNormalization(\n            epsilon = 1e-6)\n        self.layer_norm3 = keras.layers.LayerNormalization(\n            epsilon = 1e-6)\n        \n        self.dropout1 = keras.layers.Dropout(rate)\n        self.dropout2 = keras.layers.Dropout(rate)\n        self.dropout3 = keras.layers.Dropout(rate)\n    \n    \n    def call(self, x, encoding_outputs, training,\n             decoder_mask, encoder_decoder_padding_mask):\n        # decoder_mask: 由look_ahead_mask和decoder_padding_mask合并而来\n        \n        # x.shape: (batch_size, target_seq_len, d_model)\n        # encoding_outputs.shape: (batch_size, input_seq_len, d_model)\n        \n        # 按照上面类的注释的步骤依次来编写call实现\n        # attn1, out1.shape : (batch_size, target_seq_len, d_model)\n        attn1, attn_weights1 = self.mha1(x, x, x, decoder_mask)\n        attn1 = self.dropout1(attn1, training = training)\n        out1 = self.layer_norm1(attn1 + x)\n        \n        # attn2, out2.shape : (batch_size, target_seq_len, d_model)\n        attn2, attn_weights2 = self.mha2(\n            out1, encoding_outputs, encoding_outputs,\n            encoder_decoder_padding_mask) \n        attn2 = self.dropout2(attn2, training = training)\n        out2 = self.layer_norm2(attn2 + out1)\n        \n        # ffn_output, out3.shape: (batch_size, target_seq_len, d_model)\n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layer_norm3(ffn_output + out2)\n        \n        return out3, attn_weights1, attn_weights2\n\n# 测试\nsample_decoder_layer = DecoderLayer(512, 8, 2048)\nsample_decoder_input = tf.random.uniform((64, 60, 512))\nsample_decoder_output, sample_decoder_attn_weights1, sample_decoder_attn_weights2 = sample_decoder_layer(\n    sample_decoder_input, sample_output, False, None, None)\n\nprint(sample_decoder_output.shape)\nprint(sample_decoder_attn_weights1.shape)  # 最后一维60是和x的维度一致的\nprint(sample_decoder_attn_weights2.shape)  # 最后一维50是和x的维度相关的","metadata":{"id":"jdjimN-K7XEX","outputId":"e9781422-3b43-41e0-b819-4c736c43af12","execution":{"iopub.status.busy":"2022-08-06T04:01:49.524985Z","iopub.execute_input":"2022-08-06T04:01:49.525404Z","iopub.status.idle":"2022-08-06T04:01:49.601164Z","shell.execute_reply.started":"2022-08-06T04:01:49.525368Z","shell.execute_reply":"2022-08-06T04:01:49.600168Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# EncoderModel","metadata":{"id":"4tFoMPEey7xh"}},{"cell_type":"code","source":"# 我们多堆建几个EncoderLayer就是我们的EncoderModel\nclass EncoderModel(keras.layers.Layer):\n    def __init__(self, num_layers, input_vocab_size, max_length,\n                 d_model, num_heads, dff, rate=0.1):\n        super(EncoderModel, self).__init__()\n        self.d_model = d_model\n        # 这是layers数目\n        self.num_layers = num_layers\n        self.max_length = max_length\n        \n        # 构建embedding层\n        self.embedding = keras.layers.Embedding(input_vocab_size,\n                                                self.d_model)\n        # position_embedding.shape: (1, max_length, d_model)\n        self.position_embedding = get_position_embedding(max_length,\n                                                         self.d_model)\n        \n        self.dropout = keras.layers.Dropout(rate)\n        self.encoder_layers = [\n            EncoderLayer(d_model, num_heads, dff, rate)\n            for _ in range(self.num_layers)]\n        \n    \n    def call(self, x, training, encoder_padding_mask):\n        # x.shape: (batch_size, input_seq_len)\n        input_seq_len = tf.shape(x)[1]\n        tf.debugging.assert_less_equal(\n            input_seq_len, self.max_length,\n            \"input_seq_len should be less or equal to self.max_length\")\n        \n        # x.shape: (batch_size, input_seq_len, d_model)\n        x = self.embedding(x)\n        # x做缩放，是值在0到d_model之间\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        # 因为x长度比position_embedding可能要小，因此embedding切片后和x相加\n        # position_embedding的轴0的size是1，在x的轴零维度广播相加\n        x += self.position_embedding[:, :input_seq_len, :]\n        \n        x = self.dropout(x, training = training)  # 在进入encoderLayer之前进行dropout，防止过拟合\n        \n        # 得到的x不断作为下一层的输入\n        for i in range(self.num_layers):\n            x = self.encoder_layers[i](x, training,\n                                       encoder_padding_mask)\n        #x最终shape如下\n        # x.shape: (batch_size, input_seq_len, d_model)\n        return x\n\n#测试\nsample_encoder_model = EncoderModel(2, 8500, max_length,\n                                    512, 8, 2048)\nsample_encoder_model_input = tf.random.uniform((64, 37))\nsample_encoder_model_output = sample_encoder_model(\n    sample_encoder_model_input, False, encoder_padding_mask = None)\nprint(sample_encoder_model_output.shape)","metadata":{"id":"qB-67Tie7XEY","outputId":"326f3bab-27f0-4c0a-efb8-00c2564e5515","execution":{"iopub.status.busy":"2022-08-06T04:01:49.603839Z","iopub.execute_input":"2022-08-06T04:01:49.604168Z","iopub.status.idle":"2022-08-06T04:01:49.712247Z","shell.execute_reply.started":"2022-08-06T04:01:49.604134Z","shell.execute_reply":"2022-08-06T04:01:49.711208Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"#4.5 DecoderModel","metadata":{"id":"G9OcrKe-1mQg"}},{"cell_type":"code","source":"#和encodermodel类似\nclass DecoderModel(keras.layers.Layer):\n    def __init__(self, num_layers, target_vocab_size, max_length,\n                 d_model, num_heads, dff, rate=0.1):\n        super(DecoderModel, self).__init__()\n        self.num_layers = num_layers\n        self.max_length = max_length\n        self.d_model = d_model\n        \n        self.embedding = keras.layers.Embedding(target_vocab_size,\n                                                d_model)\n        self.position_embedding = get_position_embedding(max_length,\n                                                         d_model)\n        \n        self.dropout = keras.layers.Dropout(rate)\n        self.decoder_layers = [\n            DecoderLayer(d_model, num_heads, dff, rate)\n            for _ in range(self.num_layers)]\n        \n    \n    def call(self, x, encoding_outputs, training,\n             decoder_mask, encoder_decoder_padding_mask):\n        # x.shape: (batch_size, output_seq_len)\n        output_seq_len = tf.shape(x)[1]\n        #如果要输出的商都超出了max_length，就报错\n        tf.debugging.assert_less_equal(\n            output_seq_len, self.max_length,\n            \"output_seq_len should be less or equal to self.max_length\")\n        \n        #attention_weights都是由decoder layer返回，把它保存下来\n        attention_weights = {}\n        \n        # x.shape: (batch_size, output_seq_len, d_model)\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.position_embedding[:, :output_seq_len, :]\n        \n        x = self.dropout(x, training = training)\n        \n        for i in range(self.num_layers):\n            #attn1,attn2分别是两个attention\n            x, attn1, attn2 = self.decoder_layers[i](\n                x, encoding_outputs, training,\n                decoder_mask, encoder_decoder_padding_mask)\n            attention_weights[\n                'decoder_layer{}_att1'.format(i+1)] = attn1\n            attention_weights[\n                'decoder_layer{}_att2'.format(i+1)] = attn2\n        # x.shape: (batch_size, output_seq_len, d_model),attention_weights是为了画图\n        return x, attention_weights\n\nsample_decoder_model = DecoderModel(2, 8000, max_length,\n                                    512, 8, 2048)\n#测试\nsample_decoder_model_input = tf.random.uniform((64, 35))\nsample_decoder_model_output, sample_decoder_model_att \\\n= sample_decoder_model(\n    sample_decoder_model_input,\n    sample_encoder_model_output,#注意这里是encoder的output\n    training = False, decoder_mask = None,\n    encoder_decoder_padding_mask = None)\n\nprint(sample_decoder_model_output.shape)\n# for key in sample_decoder_model_att:\n#     print(sample_decoder_model_att[key].shape)","metadata":{"scrolled":true,"id":"WoezvOMa7XEY","outputId":"bc74dcc6-0dd5-496b-ba08-62f1b8cbe4ed","execution":{"iopub.status.busy":"2022-08-06T04:01:49.713470Z","iopub.execute_input":"2022-08-06T04:01:49.713771Z","iopub.status.idle":"2022-08-06T04:01:49.845122Z","shell.execute_reply.started":"2022-08-06T04:01:49.713746Z","shell.execute_reply":"2022-08-06T04:01:49.844118Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# 4.6 Transformer","metadata":{"id":"Dx4Knkoj8RM3"}},{"cell_type":"code","source":"class Transformer(keras.Model):\n    def __init__(self, num_layers, input_vocab_size, target_vocab_size,\n                 max_length, d_model, num_heads, dff, rate=0.1):\n        super(Transformer, self).__init__()\n        \n        self.encoder_model = EncoderModel(\n            num_layers, input_vocab_size, max_length,\n            d_model, num_heads, dff, rate)\n        \n        self.decoder_model = DecoderModel(\n            num_layers, target_vocab_size, max_length,\n            d_model, num_heads, dff, rate)\n        \n        self.final_layer = keras.layers.Dense(target_vocab_size)\n    \n    def call(self, inp, tar, training, encoder_padding_mask,\n             decoder_mask, encoder_decoder_padding_mask):\n        # encoding_outputs.shape: (batch_size, input_seq_len, d_model)\n        encoding_outputs = self.encoder_model(\n            inp, training, encoder_padding_mask)\n        \n        # decoding_outputs.shape: (batch_size, output_seq_len, d_model)\n        decoding_outputs, attention_weights = self.decoder_model(\n            tar, encoding_outputs, training,\n            decoder_mask, encoder_decoder_padding_mask)\n        \n        # predictions.shape: (batch_size, output_seq_len, target_vocab_size)\n        predictions = self.final_layer(decoding_outputs)\n        \n        return predictions, attention_weights\n\n#测试\nsample_transformer = Transformer(4, pt_tokenizer.vocab_size + 2, en_tokenizer.vocab_size + 2, max_length,\n                                 128, 8, 512, rate = 0.1)\ntemp_input = tf.random.uniform((64, 26))\ntemp_target = tf.random.uniform((64, 31))\n\n#得到输出\npredictions, attention_weights = sample_transformer(\n    temp_input, temp_target, training = False,\n    encoder_padding_mask = None,\n    decoder_mask = None,\n    encoder_decoder_padding_mask = None)\n#输出shape\nprint(predictions.shape)\nprint('-'*50)\n#attention_weights 的shape打印\nfor key in attention_weights:\n    print(key, attention_weights[key].shape)\nprint('-'*50)\nsample_transformer.summary()        ","metadata":{"id":"W4pb_OdD7XEZ","outputId":"f0671921-9e93-425d-f241-19929d6bd335","execution":{"iopub.status.busy":"2022-08-06T04:01:49.846366Z","iopub.execute_input":"2022-08-06T04:01:49.846679Z","iopub.status.idle":"2022-08-06T04:01:50.285782Z","shell.execute_reply.started":"2022-08-06T04:01:49.846647Z","shell.execute_reply":"2022-08-06T04:01:50.284652Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# 1. initializes model.\n# 2. define loss, optimizer, learning_rate schedule\n# 3. train_step\n# 4. train process","metadata":{"id":"8u8NnVRg7XEZ","execution":{"iopub.status.busy":"2022-08-06T04:01:50.287164Z","iopub.execute_input":"2022-08-06T04:01:50.288029Z","iopub.status.idle":"2022-08-06T04:01:50.292698Z","shell.execute_reply.started":"2022-08-06T04:01:50.287989Z","shell.execute_reply":"2022-08-06T04:01:50.291620Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"max_length","metadata":{"id":"pMSE_bpv7XEZ","outputId":"6b921a66-4315-49f5-d79e-ed8bc2bb2bb3","execution":{"iopub.status.busy":"2022-08-06T04:01:50.294289Z","iopub.execute_input":"2022-08-06T04:01:50.294846Z","iopub.status.idle":"2022-08-06T04:01:50.306717Z","shell.execute_reply.started":"2022-08-06T04:01:50.294811Z","shell.execute_reply":"2022-08-06T04:01:50.305631Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"num_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\n#加2是因为最后两个位置是start和end\ninput_vocab_size = pt_tokenizer.vocab_size + 2\ntarget_vocab_size = en_tokenizer.vocab_size + 2\n\ndropout_rate = 0.1\n\ntransformer = Transformer(num_layers,\n                          input_vocab_size,\n                          target_vocab_size,\n                          max_length,\n                          d_model, num_heads, dff, dropout_rate)","metadata":{"id":"bIpcWeFh7XEZ","execution":{"iopub.status.busy":"2022-08-06T04:01:50.308746Z","iopub.execute_input":"2022-08-06T04:01:50.309365Z","iopub.status.idle":"2022-08-06T04:01:50.402310Z","shell.execute_reply.started":"2022-08-06T04:01:50.309328Z","shell.execute_reply":"2022-08-06T04:01:50.401425Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"tf.math.rsqrt(10.0)  #1/sqrt(10)","metadata":{"id":"kDGqwfZN_KgV","outputId":"7d20e996-8d70-4f8d-a686-6b87ea23ffe9","execution":{"iopub.status.busy":"2022-08-06T04:01:50.406289Z","iopub.execute_input":"2022-08-06T04:01:50.406575Z","iopub.status.idle":"2022-08-06T04:01:50.414291Z","shell.execute_reply.started":"2022-08-06T04:01:50.406551Z","shell.execute_reply":"2022-08-06T04:01:50.413157Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#学习率变化，是先增后减，因为前期可以快点，后期模型比较好，就要慢点\n# lrate = (d_model ** -0.5) * min(step_num ** (-0.5),\n#                                 step_num * warm_up_steps **(-1.5))\n#自定义的学习率调整设计实现\n#这里的公式看这里 https://tensorflow.google.cn/tutorials/text/transformer\nclass CustomizedSchedule(\n    keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps = 4000):\n        super(CustomizedSchedule, self).__init__()\n        \n        self.d_model = tf.cast(d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n    \n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** (-1.5))\n        \n        arg3 = tf.math.rsqrt(self.d_model)\n        \n        return arg3 * tf.math.minimum(arg1, arg2)\n    \nlearning_rate = CustomizedSchedule(d_model)\noptimizer = keras.optimizers.Adam(learning_rate,\n                                  beta_1 = 0.9,\n                                  beta_2 = 0.98,\n                                  epsilon = 1e-9)","metadata":{"id":"l_P_6oTj7XEZ","execution":{"iopub.status.busy":"2022-08-06T04:01:50.415997Z","iopub.execute_input":"2022-08-06T04:01:50.416712Z","iopub.status.idle":"2022-08-06T04:01:50.425620Z","shell.execute_reply.started":"2022-08-06T04:01:50.416677Z","shell.execute_reply":"2022-08-06T04:01:50.424621Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"temp_learning_rate_schedule = CustomizedSchedule(d_model)\n#下面是学习率的设计图\nplt.plot(\n    temp_learning_rate_schedule(\n        tf.range(40000, dtype=tf.float32)))\nplt.ylabel(\"Leraning rate\")\nplt.xlabel(\"Train step\")","metadata":{"id":"6HiybSWo7XEZ","outputId":"f700b49d-21e8-4f55-b293-d8faa424f2bc","execution":{"iopub.status.busy":"2022-08-06T04:01:50.427119Z","iopub.execute_input":"2022-08-06T04:01:50.427719Z","iopub.status.idle":"2022-08-06T04:01:50.638655Z","shell.execute_reply.started":"2022-08-06T04:01:50.427684Z","shell.execute_reply":"2022-08-06T04:01:50.637736Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"loss_object = keras.losses.SparseCategoricalCrossentropy(\n    from_logits = True)\n\ndef loss_function(real, pred):\n    #损失做了掩码处理，是padding的地方不计算损失\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    \n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    \n    return tf.reduce_mean(loss_)\n","metadata":{"id":"1ETQI-Ta7XEZ","execution":{"iopub.status.busy":"2022-08-06T04:01:50.640118Z","iopub.execute_input":"2022-08-06T04:01:50.640723Z","iopub.status.idle":"2022-08-06T04:01:50.647544Z","shell.execute_reply.started":"2022-08-06T04:01:50.640687Z","shell.execute_reply":"2022-08-06T04:01:50.646506Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"## 初试化3个mask矩阵，为了训练时直接输入进去","metadata":{"id":"jkFf4o6qEAt_"}},{"cell_type":"code","source":"def create_masks(inp, tar):\n    \"\"\"\n    Encoder:\n      - encoder_padding_mask (self attention of EncoderLayer)\n      对于encoder中padding值没作用，所以无需attention\n    Decoder:\n      - look_ahead_mask (self attention of DecoderLayer)\n      target位置上的词不能看到之后的词，因为之后的词没预测出来\n      - encoder_decoder_padding_mask (encoder-decoder attention of DecoderLayer)\n      decoder不应该到encoder的padding上去花费精力\n      - decoder_padding_mask (self attention of DecoderLayer)\n      decoder也有padding，所以mask掉\n    \"\"\"\n    encoder_padding_mask = create_padding_mask(inp)\n    encoder_decoder_padding_mask = create_padding_mask(inp)\n    \n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    decoder_padding_mask = create_padding_mask(tar)\n    decoder_mask = tf.maximum(decoder_padding_mask,\n                              look_ahead_mask)\n    \n#     print( encoder_padding_mask.shape ) #这里看完要注释掉，避免下面太多打印\n#     print( encoder_decoder_padding_mask.shape )\n    print( look_ahead_mask.shape)\n    print('-'*50)\n    print( decoder_padding_mask.shape)\n    print('-'*50)\n    print( decoder_mask.shape)\n\n    \n    return encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask","metadata":{"id":"jrhd0X4P7XEa","execution":{"iopub.status.busy":"2022-08-06T04:01:50.649060Z","iopub.execute_input":"2022-08-06T04:01:50.649724Z","iopub.status.idle":"2022-08-06T04:01:50.660500Z","shell.execute_reply.started":"2022-08-06T04:01:50.649686Z","shell.execute_reply":"2022-08-06T04:01:50.659399Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"temp_inp, temp_tar = iter(train_dataset.take(1)).next()","metadata":{"id":"IvwIZ9ZJ7XEa","execution":{"iopub.status.busy":"2022-08-06T04:01:50.663963Z","iopub.execute_input":"2022-08-06T04:01:50.664256Z","iopub.status.idle":"2022-08-06T04:02:14.371264Z","shell.execute_reply.started":"2022-08-06T04:01:50.664226Z","shell.execute_reply":"2022-08-06T04:02:14.370267Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"print(temp_inp.shape)\nprint(temp_tar.shape)\ncreate_masks(temp_inp, temp_tar)\n#样本大小是64，不足的补齐35，或者39\n#最后是(64, 1, 39, 39)原因是既不关注前面的padding，也不关注后面的单词","metadata":{"id":"qax-L4117XEa","outputId":"a3283164-1312-45a7-9193-325a853b83dc","execution":{"iopub.status.busy":"2022-08-06T04:02:14.374018Z","iopub.execute_input":"2022-08-06T04:02:14.374867Z","iopub.status.idle":"2022-08-06T04:02:14.394248Z","shell.execute_reply.started":"2022-08-06T04:02:14.374809Z","shell.execute_reply":"2022-08-06T04:02:14.393233Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"train_loss = keras.metrics.Mean(name = 'train_loss')\ntrain_accuracy = keras.metrics.SparseCategoricalAccuracy(\n    name = 'train_accuracy')\n\n@tf.function\ndef train_step(inp, tar):\n    tar_inp  = tar[:, :-1]  #没带end\n    tar_real = tar[:, 1:]   #没有start\n    \n    encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n    = create_masks(inp, tar_inp)\n    \n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(inp, tar_inp, True,\n                                     encoder_padding_mask,\n                                     decoder_mask,\n                                     encoder_decoder_padding_mask)\n        loss = loss_function(tar_real, predictions)\n    \n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    optimizer.apply_gradients(\n        zip(gradients, transformer.trainable_variables))\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)\n#一个epochs接近90秒\nepochs = 20\nfor epoch in range(epochs):\n    start = time.time()\n    #reset后就会从零开始累计\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    \n    for (batch, (inp, tar)) in enumerate(train_dataset):\n        train_step(inp, tar)\n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n                epoch + 1, batch, train_loss.result(),\n                train_accuracy.result()))\n    \n    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(\n        epoch + 1, train_loss.result(), train_accuracy.result()))\n    print('Time take for 1 epoch: {} secs\\n'.format(\n        time.time() - start))\n\n#loss是一个正常的指标，accuracy只是机器翻译的一个参考指标，可以看趋势\n    ","metadata":{"id":"eL69dUcK7XEa","outputId":"4443153f-f61e-4242-ae3a-dc55ca06950d","execution":{"iopub.status.busy":"2022-08-06T04:02:14.395867Z","iopub.execute_input":"2022-08-06T04:02:14.396281Z","iopub.status.idle":"2022-08-06T04:41:08.449579Z","shell.execute_reply.started":"2022-08-06T04:02:14.396246Z","shell.execute_reply":"2022-08-06T04:41:08.448476Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"tf.expand_dims([en_tokenizer.vocab_size], 0)","metadata":{"id":"mGpfvVXqFyw2","outputId":"69de5220-dca1-4b67-853d-d7c5c9621aeb","execution":{"iopub.status.busy":"2022-08-06T04:41:08.454507Z","iopub.execute_input":"2022-08-06T04:41:08.454790Z","iopub.status.idle":"2022-08-06T04:41:08.467284Z","shell.execute_reply.started":"2022-08-06T04:41:08.454763Z","shell.execute_reply":"2022-08-06T04:41:08.466142Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"\"\"\"\neg: A B C D -> E F G H.\nTrain: A B C D, E F G -> F G H，相当于给了EFG，得到FGH\nEval:  A B C D -> E\n       A B C D, E -> F\n       A B C D, E F -> G\n       A B C D, E F G -> H\n类似seq2seq2\n不同的是 transformer可以并行的处理，前后没有依赖，而seq2seq前后有依赖\n\"\"\"\ndef evaluate(inp_sentence):\n    #文本的句子转换为id的句子\n    input_id_sentence = [pt_tokenizer.vocab_size] \\\n    + pt_tokenizer.encode(inp_sentence) + [pt_tokenizer.vocab_size + 1]\n    #transformer转换是两维的，因此转换\n    # encoder_input.shape: (1, input_sentence_length)\n    encoder_input = tf.expand_dims(input_id_sentence, 0)\n    \n    # decoder_input.shape: (1, 1)\n    #我们预测一个词就放入decoder_input，decoder_input给多个就可以预测多个，我们给一个\n    decoder_input = tf.expand_dims([en_tokenizer.vocab_size], 0)  #做一个<start>传进去，去预测i\n    \n    for i in range(max_length):\n        #产生mask并传给transformer\n        encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n        = create_masks(encoder_input, decoder_input)\n        # predictions.shape: (batch_size, output_target_len, target_vocab_size)\n        predictions, attention_weights = transformer(\n            encoder_input,\n            decoder_input,\n            False,\n            encoder_padding_mask,\n            decoder_mask,\n            encoder_decoder_padding_mask)\n        # predictions.shape: (batch_size, target_vocab_size)\n        #我们每次只预测一个，所以是最后一个，第三个维度是密集向量维度\n        print(predictions.shape)\n        predictions = predictions[:, -1, :]\n        #预测值就是概率最大的那个的索引，那最后一个维度中最大的那个值\n        predicted_id = tf.cast(tf.argmax(predictions, axis = -1),\n                               tf.int32)\n        #如果等于end id，预测结束\n        if tf.equal(predicted_id, en_tokenizer.vocab_size + 1):\n            return tf.squeeze(decoder_input, axis = 0), attention_weights\n        #如果predicted_id不是end id，添加到新的decoder_input中\n        decoder_input = tf.concat([decoder_input, [predicted_id]],\n                                  axis = -1)\n        # print(decoder_input)\n    return tf.squeeze(decoder_input, axis = 0), attention_weights\n        \n        ","metadata":{"id":"j8PkUTox7XEc","execution":{"iopub.status.busy":"2022-08-06T04:41:08.468758Z","iopub.execute_input":"2022-08-06T04:41:08.469284Z","iopub.status.idle":"2022-08-06T04:41:08.480043Z","shell.execute_reply.started":"2022-08-06T04:41:08.469249Z","shell.execute_reply":"2022-08-06T04:41:08.479036Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"[np.random.randint(0,en_tokenizer.vocab_size-1) for i in range(5-1)]","metadata":{"id":"yw2xlFptKg74","outputId":"33babfa2-001f-43d9-ce68-3f57eaa4f541","execution":{"iopub.status.busy":"2022-08-06T04:41:08.482161Z","iopub.execute_input":"2022-08-06T04:41:08.483125Z","iopub.status.idle":"2022-08-06T04:41:08.496134Z","shell.execute_reply.started":"2022-08-06T04:41:08.483090Z","shell.execute_reply":"2022-08-06T04:41:08.495049Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def evaluate1(inp_sentence):\n    #文本的句子转换为id的句子\n    input_id_sentence = [pt_tokenizer.vocab_size] \\\n    + pt_tokenizer.encode(inp_sentence) + [pt_tokenizer.vocab_size + 1]\n    #transformer转换是两维的，因此转换\n    # encoder_input.shape: (1, input_sentence_length)\n    encoder_input = tf.expand_dims(input_id_sentence, 0)\n    \n    # decoder_input.shape: (1, 1)\n    #我们预测一个词就放入decoder_input，decoder_input给多个就可以预测多个，我们给一个\n    decoder_input = tf.expand_dims([en_tokenizer.vocab_size]+[np.random.randint(0,en_tokenizer.vocab_size-1) for i in range(len(input_id_sentence)-1)], 0)  #做一个<start>传进去，去预测i\n\n    \n        #产生mask并传给transformer\n    encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n    = create_masks(encoder_input, decoder_input)\n    # predictions.shape: (batch_size, output_target_len, target_vocab_size)\n    predictions, attention_weights = transformer(\n        encoder_input,\n        decoder_input,\n        False,\n        encoder_padding_mask,\n        decoder_mask,\n        encoder_decoder_padding_mask)\n    # predictions.shape: (batch_size, target_vocab_size)\n    #我们每次只预测一个，所以是最后一个，第三个维度是密集向量维度\n    print(predictions.shape)\n    #预测值就是概率最大的那个的索引，那最后一个维度中最大的那个值\n    predicted_id = tf.cast(tf.argmax(predictions, axis = -1),\n                            tf.int32)\n    print(predicted_id)\n    predicted_id=tf.squeeze(predicted_id)\n    return predicted_id,None\n","metadata":{"id":"8oWp4npCJbUh","execution":{"iopub.status.busy":"2022-08-06T04:41:08.499788Z","iopub.execute_input":"2022-08-06T04:41:08.501562Z","iopub.status.idle":"2022-08-06T04:41:08.509492Z","shell.execute_reply.started":"2022-08-06T04:41:08.501522Z","shell.execute_reply":"2022-08-06T04:41:08.508448Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def plot_encoder_decoder_attention(attention, input_sentence,\n                                   result, layer_name):\n    fig = plt.figure(figsize = (16, 8))\n    \n    input_id_sentence = pt_tokenizer.encode(input_sentence)\n    \n    # attention.shape: (num_heads, tar_len, input_len)\n    attention = tf.squeeze(attention[layer_name], axis = 0)\n    \n    for head in range(attention.shape[0]):\n        ax = fig.add_subplot(2, 4, head + 1)\n        \n        ax.matshow(attention[head][:-1, :])\n        \n        fontdict = {'fontsize': 10}\n        \n        ax.set_xticks(range(len(input_id_sentence) + 2))\n        ax.set_yticks(range(len(result)))\n        \n        ax.set_ylim(len(result) - 1.5, -0.5)\n        \n        ax.set_xticklabels(\n            ['<start>'] + [pt_tokenizer.decode([i]) for i in input_id_sentence] + ['<end>'],\n            fontdict = fontdict, rotation = 90)\n        ax.set_yticklabels(\n            [en_tokenizer.decode([i]) for i in result if i < en_tokenizer.vocab_size],\n            fontdict = fontdict)\n        ax.set_xlabel('Head {}'.format(head + 1))\n    plt.tight_layout()\n    plt.show()      ","metadata":{"id":"B1YjyfkQ7XEc","execution":{"iopub.status.busy":"2022-08-06T04:41:08.516653Z","iopub.execute_input":"2022-08-06T04:41:08.517569Z","iopub.status.idle":"2022-08-06T04:41:08.527529Z","shell.execute_reply.started":"2022-08-06T04:41:08.517540Z","shell.execute_reply":"2022-08-06T04:41:08.526410Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"def translate(input_sentence, layer_name = ''):\n    result, attention_weights = evaluate(input_sentence)\n    \n    predicted_sentence = en_tokenizer.decode(\n        [i for i in result if i < en_tokenizer.vocab_size])\n    \n    print(\"Input: {}\".format(input_sentence))\n    print(\"Predicted translation: {}\".format(predicted_sentence))\n    \n    if layer_name:\n        plot_encoder_decoder_attention(attention_weights, input_sentence,\n                                       result, layer_name)","metadata":{"id":"ICxvSCaM7XEd","execution":{"iopub.status.busy":"2022-08-06T04:41:08.529100Z","iopub.execute_input":"2022-08-06T04:41:08.529511Z","iopub.status.idle":"2022-08-06T04:41:08.541366Z","shell.execute_reply.started":"2022-08-06T04:41:08.529475Z","shell.execute_reply":"2022-08-06T04:41:08.540225Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"translate('está muito frio aqui.')  #预测时，不可以并行","metadata":{"id":"BFKhvflZK5n0","outputId":"17f6049f-04d8-407c-d916-24a847b7e037","execution":{"iopub.status.busy":"2022-08-06T04:41:08.542542Z","iopub.execute_input":"2022-08-06T04:41:08.543681Z","iopub.status.idle":"2022-08-06T04:41:09.386704Z","shell.execute_reply.started":"2022-08-06T04:41:08.543646Z","shell.execute_reply":"2022-08-06T04:41:09.385673Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"translate('está muito frio aqui.')","metadata":{"id":"OZ7ZZALs7XEd","outputId":"41e9bf60-3f1e-4cf1-b3ca-432d3285f50e","execution":{"iopub.status.busy":"2022-08-06T04:41:09.388407Z","iopub.execute_input":"2022-08-06T04:41:09.388798Z","iopub.status.idle":"2022-08-06T04:41:10.259831Z","shell.execute_reply.started":"2022-08-06T04:41:09.388758Z","shell.execute_reply":"2022-08-06T04:41:10.258760Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"translate('isto é minha vida')","metadata":{"id":"2dEtKxqn7XEd","outputId":"3d107a6f-f192-4a3f-c75e-afcce62ef794","execution":{"iopub.status.busy":"2022-08-06T04:41:10.261530Z","iopub.execute_input":"2022-08-06T04:41:10.261912Z","iopub.status.idle":"2022-08-06T04:41:11.095907Z","shell.execute_reply.started":"2022-08-06T04:41:10.261876Z","shell.execute_reply":"2022-08-06T04:41:11.094789Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"translate('você ainda está em casa?')","metadata":{"id":"8z-hQm-G7XEd","outputId":"9890c34d-08f0-40d0-d81c-7d253cab0861","execution":{"iopub.status.busy":"2022-08-06T04:41:11.097281Z","iopub.execute_input":"2022-08-06T04:41:11.097905Z","iopub.status.idle":"2022-08-06T04:41:11.724447Z","shell.execute_reply.started":"2022-08-06T04:41:11.097866Z","shell.execute_reply":"2022-08-06T04:41:11.723370Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"translate('este é o primeiro livro que eu já li')","metadata":{"id":"M9F8wT_a7XEd","outputId":"089688f5-c7ad-4f0a-deff-2661c4f4d96c","execution":{"iopub.status.busy":"2022-08-06T04:41:11.729528Z","iopub.execute_input":"2022-08-06T04:41:11.729807Z","iopub.status.idle":"2022-08-06T04:41:13.631980Z","shell.execute_reply.started":"2022-08-06T04:41:11.729781Z","shell.execute_reply":"2022-08-06T04:41:13.630907Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"translate('este é o primeiro livro que eu já li',layer_name='decoder_layer1_att2')","metadata":{"id":"1bFvwYHS7XEd","outputId":"ed8c23f0-c36e-44f9-b665-3f1aa6f65d49","execution":{"iopub.status.busy":"2022-08-06T04:41:13.633579Z","iopub.execute_input":"2022-08-06T04:41:13.633921Z","iopub.status.idle":"2022-08-06T04:41:15.932890Z","shell.execute_reply.started":"2022-08-06T04:41:13.633885Z","shell.execute_reply":"2022-08-06T04:41:15.928811Z"},"trusted":true},"execution_count":71,"outputs":[]}]}
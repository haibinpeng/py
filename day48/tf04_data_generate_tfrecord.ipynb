{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "sys.version_info(major=3, minor=9, micro=7, releaselevel='final', serial=0)\n",
      "matplotlib 3.5.2\n",
      "numpy 1.23.0\n",
      "pandas 1.4.3\n",
      "sklearn 1.1.1\n",
      "tensorflow 2.9.1\n",
      "keras.api._v2.keras 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_00.csv\n",
      "test_01.csv\n",
      "test_02.csv\n",
      "test_03.csv\n",
      "test_04.csv\n",
      "test_05.csv\n",
      "test_06.csv\n",
      "test_07.csv\n",
      "test_08.csv\n",
      "test_09.csv\n",
      "train_00.csv\n",
      "train_01.csv\n",
      "train_02.csv\n",
      "train_03.csv\n",
      "train_04.csv\n",
      "train_05.csv\n",
      "train_06.csv\n",
      "train_07.csv\n",
      "train_08.csv\n",
      "train_09.csv\n",
      "train_10.csv\n",
      "train_11.csv\n",
      "train_12.csv\n",
      "train_13.csv\n",
      "train_14.csv\n",
      "train_15.csv\n",
      "train_16.csv\n",
      "train_17.csv\n",
      "train_18.csv\n",
      "train_19.csv\n",
      "valid_00.csv\n",
      "valid_01.csv\n",
      "valid_02.csv\n",
      "valid_03.csv\n",
      "valid_04.csv\n",
      "valid_05.csv\n",
      "valid_06.csv\n",
      "valid_07.csv\n",
      "valid_08.csv\n",
      "valid_09.csv\n"
     ]
    }
   ],
   "source": [
    "!ls generate_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./generate_csv/train_00.csv',\n",
      " './generate_csv/train_01.csv',\n",
      " './generate_csv/train_02.csv',\n",
      " './generate_csv/train_03.csv',\n",
      " './generate_csv/train_04.csv',\n",
      " './generate_csv/train_05.csv',\n",
      " './generate_csv/train_06.csv',\n",
      " './generate_csv/train_07.csv',\n",
      " './generate_csv/train_08.csv',\n",
      " './generate_csv/train_09.csv',\n",
      " './generate_csv/train_10.csv',\n",
      " './generate_csv/train_11.csv',\n",
      " './generate_csv/train_12.csv',\n",
      " './generate_csv/train_13.csv',\n",
      " './generate_csv/train_14.csv',\n",
      " './generate_csv/train_15.csv',\n",
      " './generate_csv/train_16.csv',\n",
      " './generate_csv/train_17.csv',\n",
      " './generate_csv/train_18.csv',\n",
      " './generate_csv/train_19.csv']\n",
      "['./generate_csv/valid_00.csv',\n",
      " './generate_csv/valid_01.csv',\n",
      " './generate_csv/valid_02.csv',\n",
      " './generate_csv/valid_03.csv',\n",
      " './generate_csv/valid_04.csv',\n",
      " './generate_csv/valid_05.csv',\n",
      " './generate_csv/valid_06.csv',\n",
      " './generate_csv/valid_07.csv',\n",
      " './generate_csv/valid_08.csv',\n",
      " './generate_csv/valid_09.csv']\n",
      "['./generate_csv/test_00.csv',\n",
      " './generate_csv/test_01.csv',\n",
      " './generate_csv/test_02.csv',\n",
      " './generate_csv/test_03.csv',\n",
      " './generate_csv/test_04.csv',\n",
      " './generate_csv/test_05.csv',\n",
      " './generate_csv/test_06.csv',\n",
      " './generate_csv/test_07.csv',\n",
      " './generate_csv/test_08.csv',\n",
      " './generate_csv/test_09.csv']\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"./generate_csv/\"\n",
    "\n",
    "# 通过判断开头去添加文件\n",
    "def get_filenames_by_prefix(source_dir, prefix_name):\n",
    "    all_files = os.listdir(source_dir)\n",
    "    results = []\n",
    "    for filename in all_files:\n",
    "        if filename.startswith(prefix_name):\n",
    "            results.append(os.path.join(source_dir, filename))\n",
    "    return results\n",
    "\n",
    "train_filenames = get_filenames_by_prefix(source_dir, \"train\")\n",
    "valid_filenames = get_filenames_by_prefix(source_dir, \"valid\")\n",
    "test_filenames = get_filenames_by_prefix(source_dir, \"test\")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(train_filenames)\n",
    "pprint.pprint(valid_filenames)\n",
    "pprint.pprint(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_line(line, n_fields = 9):\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:])\n",
    "    return x, y\n",
    "\n",
    "def csv_reader_dataset(filenames, n_readers=5,\n",
    "                       batch_size=32, n_parse_threads=5,\n",
    "                       shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line,\n",
    "                          num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 32\n",
    "train_set = csv_reader_dataset(train_filenames,\n",
    "                               batch_size = batch_size)\n",
    "valid_set = csv_reader_dataset(valid_filenames,\n",
    "                               batch_size = batch_size)\n",
    "test_set = csv_reader_dataset(test_filenames,\n",
    "                              batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_csv\n",
      "generate_tfrecords\n",
      "temp.csv\n",
      "tf01-dataset_basic_api.ipynb\n",
      "tf02_data_generate_csv.ipynb\n",
      "tf03-tfrecord_basic_api.ipynb\n",
      "tf04_data_generate_tfrecord.ipynb\n",
      "tfrecord_basic\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把train_set,valid_set,test_set 存储到tfrecord类型的文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把基础的如何序列化的步骤搞到一个函数\n",
    "def serialize_example(x, y):\n",
    "    \"\"\"Converts x, y to tf.train.Example and serialize\"\"\"\n",
    "    input_feautres = tf.train.FloatList(value = x)  # 特征\n",
    "    label = tf.train.FloatList(value = y)  # 标签\n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            \"input_features\": tf.train.Feature(\n",
    "                float_list = input_feautres),\n",
    "            \"label\": tf.train.Feature(float_list = label)\n",
    "        }\n",
    "    )\n",
    "    # 把features变为example\n",
    "    example = tf.train.Example(features = features)\n",
    "    return example.SerializeToString()  # 把example序列化\n",
    "\n",
    "# n_shards是存为多少个文件，steps_per_shard和 steps_per_epoch类似\n",
    "def csv_dataset_to_tfrecords(base_filename, dataset,\n",
    "                             n_shards, steps_per_shard,\n",
    "                             compression_type = None):\n",
    "    # 压缩文件类型\n",
    "    options = tf.io.TFRecordOptions(\n",
    "        compression_type = compression_type)\n",
    "    all_filenames = []\n",
    "    \n",
    "    for shard_id in range(n_shards):\n",
    "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(\n",
    "            base_filename, shard_id, n_shards)  # base_filename是一个前缀\n",
    "        # 打开文件\n",
    "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer:\n",
    "            # 取出数据,为什么skip，上一个文件写了前500行，下一个文件存后面的数据\n",
    "            for x_batch, y_batch in dataset.skip(shard_id * steps_per_shard).take(steps_per_shard):\n",
    "                for x_example, y_example in zip(x_batch, y_batch):\n",
    "                    writer.write(\n",
    "                        serialize_example(x_example, y_example))\n",
    "        all_filenames.append(filename_fullpath)\n",
    "    # 返回所有tfrecord文件名\n",
    "    return all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf generate_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 8), dtype=float32, numpy=\n",
      "array([[-1.05917811e+00,  1.39356470e+00, -2.63319686e-02,\n",
      "        -1.10067599e-01, -6.13819897e-01, -9.69593525e-02,\n",
      "         3.24713111e-01, -3.74772437e-02],\n",
      "       [-1.07750773e+00, -4.48740691e-01, -5.68056822e-01,\n",
      "        -1.42692626e-01, -9.66667682e-02,  1.23264685e-01,\n",
      "        -3.14486384e-01, -4.81895894e-01],\n",
      "       [ 8.01544309e-01,  2.72161424e-01, -1.16243929e-01,\n",
      "        -2.02311516e-01, -5.43051600e-01, -2.10396163e-02,\n",
      "        -5.89762092e-01, -8.24184567e-02],\n",
      "       [ 4.85305160e-01, -8.49241912e-01, -6.53012618e-02,\n",
      "        -2.33796556e-02,  1.49743509e+00, -7.79065788e-02,\n",
      "        -9.02363241e-01,  7.81451464e-01],\n",
      "       [-1.11997497e+00, -1.32984328e+00,  1.41900450e-01,\n",
      "         4.65813696e-01, -1.03017777e-01, -1.07441843e-01,\n",
      "        -7.95052409e-01,  1.53047168e+00],\n",
      "       [-2.22356573e-01,  1.39356470e+00,  2.99129952e-02,\n",
      "         8.01452026e-02, -5.09481966e-01, -6.23859912e-02,\n",
      "        -8.65037739e-01,  8.61346960e-01],\n",
      "       [-1.23107159e+00,  9.12963331e-01, -1.91945627e-01,\n",
      "         1.28514633e-01, -1.87395394e-01,  1.46042794e-01,\n",
      "        -7.85721004e-01,  6.56614780e-01],\n",
      "       [-2.98072815e-01,  3.52261662e-01, -1.09205075e-01,\n",
      "        -2.50555217e-01, -3.40640247e-02, -6.03400404e-03,\n",
      "         1.08055484e+00, -1.06113815e+00],\n",
      "       [-7.43205428e-01,  9.12963331e-01, -6.44320250e-01,\n",
      "        -1.47909701e-01,  7.39851117e-01,  1.14276908e-01,\n",
      "        -7.95052409e-01,  6.81582153e-01],\n",
      "       [-9.49093878e-01,  6.72662616e-01,  2.83705562e-01,\n",
      "         1.06555298e-01, -6.54647768e-01, -6.23949282e-02,\n",
      "         2.12736562e-01,  2.47049774e-03],\n",
      "       [ 1.99384451e-01,  1.07316375e+00, -1.98408544e-01,\n",
      "        -2.93289065e-01, -7.85210505e-02,  1.88048892e-02,\n",
      "         8.00613463e-01, -1.15102065e+00],\n",
      "       [-4.79663879e-01,  1.87416613e+00,  5.60470559e-02,\n",
      "        -6.84981234e-03,  2.94460077e-02, -1.21153988e-01,\n",
      "         1.03389800e+00, -1.34077239e+00],\n",
      "       [-3.05882934e-02, -9.29342151e-01,  2.59621471e-01,\n",
      "        -6.01274054e-03, -5.00409126e-01, -3.07798684e-02,\n",
      "         1.59844637e+00, -1.81515181e+00],\n",
      "       [-8.24676275e-01, -4.82395217e-02, -3.44865829e-01,\n",
      "        -8.47758725e-02,  5.01234829e-01, -3.46999951e-02,\n",
      "         5.30003488e-01, -8.74119252e-02],\n",
      "       [ 3.87431264e+00, -8.49241912e-01,  1.22548103e+00,\n",
      "        -2.35879254e-02,  1.02028906e-01,  3.33571471e-02,\n",
      "        -1.22896159e+00,  1.17094195e+00],\n",
      "       [-5.72590768e-01,  7.52762854e-01, -3.00921530e-01,\n",
      "        -2.63413310e-01, -2.98991591e-01, -4.50379625e-02,\n",
      "        -1.35493517e+00,  1.25083745e+00],\n",
      "       [-6.05413556e-01,  5.92562377e-01, -1.92066818e-01,\n",
      "        -3.20176035e-01, -1.14095318e+00, -1.19390488e-01,\n",
      "         1.10388327e+00, -8.71386409e-01],\n",
      "       [ 1.90638328e+00,  5.12462139e-01,  4.47582811e-01,\n",
      "        -2.76721776e-01, -6.31058335e-01, -7.08114654e-02,\n",
      "        -7.06404328e-01,  7.46497214e-01],\n",
      "       [ 7.00647458e-02,  3.18607129e-02, -2.57098645e-01,\n",
      "        -3.00019473e-01, -2.66329288e-01, -9.85835046e-02,\n",
      "         1.08522058e+00, -1.37073314e+00],\n",
      "       [ 9.98321295e-01,  9.12963331e-01,  2.77845651e-01,\n",
      "        -3.85383785e-01, -8.88727605e-01, -4.53227460e-02,\n",
      "        -6.83075845e-01,  5.76719284e-01],\n",
      "       [-2.11166978e-01,  1.55376518e+00, -1.54983953e-01,\n",
      "        -1.24784604e-01, -5.85694015e-01,  6.52302057e-02,\n",
      "        -7.76389658e-01,  6.41634405e-01],\n",
      "       [-1.23011243e+00,  2.72161424e-01, -1.86957195e-01,\n",
      "         9.53920037e-02, -1.68255866e-02,  4.41081300e-02,\n",
      "         3.01384658e-01,  2.17189625e-01],\n",
      "       [ 3.88017392e+00, -9.29342151e-01,  1.29029870e+00,\n",
      "        -1.72691330e-01, -2.25501403e-01,  5.14101014e-02,\n",
      "        -8.46374989e-01,  8.86314332e-01],\n",
      "       [ 1.23452818e+00, -1.32984328e+00,  5.75435698e-01,\n",
      "        -1.19361848e-01, -4.16938782e-01,  1.54011786e-01,\n",
      "         1.09921765e+00, -1.34576583e+00],\n",
      "       [-5.07637858e-01,  1.71396565e+00, -1.28308967e-01,\n",
      "         9.23017412e-02,  7.55274951e-01,  4.58156280e-02,\n",
      "         9.73243952e-01, -1.42066789e+00],\n",
      "       [ 1.67147771e-01,  5.12462139e-01,  6.68075010e-02,\n",
      "        -2.39582434e-01, -5.85694015e-01, -1.49112299e-01,\n",
      "        -6.97072923e-01,  6.11673594e-01],\n",
      "       [ 5.81642210e-01,  4.32361901e-01,  5.06965518e-02,\n",
      "        -1.95470005e-01, -6.59184217e-01,  1.73770804e-02,\n",
      "        -6.41084671e-01,  5.46758473e-01],\n",
      "       [ 3.72503400e-01, -6.89041436e-01,  6.45801365e-01,\n",
      "         8.00678432e-02, -3.15322757e-01, -2.51115970e-02,\n",
      "         5.62663257e-01, -5.74511178e-02],\n",
      "       [ 1.58142820e-01,  1.15326405e+00, -9.75820422e-02,\n",
      "        -2.74813175e-01, -6.60091519e-01, -9.36449692e-02,\n",
      "        -8.51040661e-01,  7.26523340e-01],\n",
      "       [-6.84433401e-01,  4.32361901e-01, -9.17521179e-01,\n",
      "        -1.76076740e-01,  5.63837588e-01,  1.37098923e-01,\n",
      "        -7.43729830e-01,  7.76458025e-01],\n",
      "       [ 1.51482344e-01,  2.72161424e-01,  2.97763169e-01,\n",
      "        -2.25783169e-01, -2.08262965e-01,  5.33962436e-03,\n",
      "        -6.73744500e-01,  3.57006729e-01],\n",
      "       [ 1.01782310e+00,  1.47366500e+00, -8.49439502e-02,\n",
      "        -1.76463351e-01, -6.39223933e-01, -9.69866440e-02,\n",
      "        -7.43729830e-01,  5.91699719e-01]], dtype=float32)>, <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[0.672  ],\n",
      "       [0.978  ],\n",
      "       [3.226  ],\n",
      "       [2.956  ],\n",
      "       [0.66   ],\n",
      "       [2.     ],\n",
      "       [0.953  ],\n",
      "       [1.514  ],\n",
      "       [1.438  ],\n",
      "       [0.607  ],\n",
      "       [1.99   ],\n",
      "       [2.895  ],\n",
      "       [1.598  ],\n",
      "       [0.717  ],\n",
      "       [5.00001],\n",
      "       [1.139  ],\n",
      "       [1.028  ],\n",
      "       [5.00001],\n",
      "       [1.61   ],\n",
      "       [3.824  ],\n",
      "       [0.931  ],\n",
      "       [0.483  ],\n",
      "       [5.00001],\n",
      "       [2.487  ],\n",
      "       [2.464  ],\n",
      "       [5.00001],\n",
      "       [2.237  ],\n",
      "       [1.431  ],\n",
      "       [2.26   ],\n",
      "       [1.563  ],\n",
      "       [2.646  ],\n",
      "       [4.741  ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for i in train_set.take(1):\n",
    "    print(i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 41.6 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 训练集和测试集都分20\n",
    "n_shards = 20\n",
    "train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "valid_steps_per_shard = 3880 // batch_size // 10\n",
    "test_steps_per_shard = 5170 // batch_size // 10\n",
    "\n",
    "output_dir = \"generate_tfrecords\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir, \"train\")\n",
    "valid_basename = os.path.join(output_dir, \"valid\")\n",
    "test_basename = os.path.join(output_dir, \"test\")\n",
    "\n",
    "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "    train_basename, train_set, n_shards, train_steps_per_shard, None)\n",
    "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "    valid_basename, valid_set, 10, valid_steps_per_shard, None)\n",
    "test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
    "    test_basename, test_set, 10, test_steps_per_shard, None)\n",
    "# 执行会发现目录下总计生成了60个文件,这里文件数目改为一致，为了对比时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1960\n",
      "-rw-r--r-- 1 yf 197121 47616 Jul 28 00:02 test_00000-of-00010\n",
      "-rw-r--r-- 1 yf 197121 47616 Jul 28 00:02 test_00001-of-00010\n",
      "-rw-r--r-- 1 yf 197121 47616 Jul 28 00:02 test_00002-of-00010\n",
      "-rw-r--r-- 1 yf 197121 47616 Jul 28 00:02 test_00003-of-00010\n",
      "-rw-r--r-- 1 yf 197121 47616 Jul 28 00:02 test_00004-of-00010\n",
      "-rw-r--r-- 1 yf 197121 47616 Jul 28 00:02 test_00005-of-00010\n",
      "-rw-r--r-- 1 yf 197121 47616 Jul 28 00:02 test_00006-of-00010\n",
      "-rw-r--r-- 1 yf 197121 47616 Jul 28 00:02 test_00007-of-00010\n",
      "-rw-r--r-- 1 yf 197121 47616 Jul 28 00:02 test_00008-of-00010\n",
      "-rw-r--r-- 1 yf 197121 47616 Jul 28 00:02 test_00009-of-00010\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00000-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00001-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00002-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00003-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00004-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00005-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00006-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00007-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00008-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00009-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00010-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00011-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00012-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00013-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00014-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00015-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00016-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00017-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00018-of-00020\n",
      "-rw-r--r-- 1 yf 197121 53568 Jul 28 00:01 train_00019-of-00020\n",
      "-rw-r--r-- 1 yf 197121 35712 Jul 28 00:01 valid_00000-of-00010\n",
      "-rw-r--r-- 1 yf 197121 35712 Jul 28 00:01 valid_00001-of-00010\n",
      "-rw-r--r-- 1 yf 197121 35712 Jul 28 00:01 valid_00002-of-00010\n",
      "-rw-r--r-- 1 yf 197121 35712 Jul 28 00:01 valid_00003-of-00010\n",
      "-rw-r--r-- 1 yf 197121 35712 Jul 28 00:01 valid_00004-of-00010\n",
      "-rw-r--r-- 1 yf 197121 35712 Jul 28 00:01 valid_00005-of-00010\n",
      "-rw-r--r-- 1 yf 197121 35712 Jul 28 00:02 valid_00006-of-00010\n",
      "-rw-r--r-- 1 yf 197121 35712 Jul 28 00:02 valid_00007-of-00010\n",
      "-rw-r--r-- 1 yf 197121 35712 Jul 28 00:02 valid_00008-of-00010\n",
      "-rw-r--r-- 1 yf 197121 35712 Jul 28 00:02 valid_00009-of-00010\n"
     ]
    }
   ],
   "source": [
    "!ls -l generate_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一下压缩的\n",
    "# n_shards = 20\n",
    "# train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "# valid_steps_per_shard = 3880 // batch_size // n_shards\n",
    "# test_steps_per_shard = 5170 // batch_size // n_shards\n",
    "\n",
    "# output_dir = \"generate_tfrecords_zip\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.mkdir(output_dir)\n",
    "\n",
    "# train_basename = os.path.join(output_dir, \"train\")\n",
    "# valid_basename = os.path.join(output_dir, \"valid\")\n",
    "# test_basename = os.path.join(output_dir, \"test\")\n",
    "# 只需修改参数的类型即可\n",
    "# train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "#     train_basename, train_set, n_shards, train_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")\n",
    "# valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "#     valid_basename, valid_set, n_shards, valid_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")\n",
    "# test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
    "#     test_basename, test_set, n_shards, test_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'generate_tfrecords_zip': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -l generate_tfrecords_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_tfrecords\\\\train_00000-of-00020',\n",
      " 'generate_tfrecords\\\\train_00001-of-00020',\n",
      " 'generate_tfrecords\\\\train_00002-of-00020',\n",
      " 'generate_tfrecords\\\\train_00003-of-00020',\n",
      " 'generate_tfrecords\\\\train_00004-of-00020',\n",
      " 'generate_tfrecords\\\\train_00005-of-00020',\n",
      " 'generate_tfrecords\\\\train_00006-of-00020',\n",
      " 'generate_tfrecords\\\\train_00007-of-00020',\n",
      " 'generate_tfrecords\\\\train_00008-of-00020',\n",
      " 'generate_tfrecords\\\\train_00009-of-00020',\n",
      " 'generate_tfrecords\\\\train_00010-of-00020',\n",
      " 'generate_tfrecords\\\\train_00011-of-00020',\n",
      " 'generate_tfrecords\\\\train_00012-of-00020',\n",
      " 'generate_tfrecords\\\\train_00013-of-00020',\n",
      " 'generate_tfrecords\\\\train_00014-of-00020',\n",
      " 'generate_tfrecords\\\\train_00015-of-00020',\n",
      " 'generate_tfrecords\\\\train_00016-of-00020',\n",
      " 'generate_tfrecords\\\\train_00017-of-00020',\n",
      " 'generate_tfrecords\\\\train_00018-of-00020',\n",
      " 'generate_tfrecords\\\\train_00019-of-00020']\n",
      "['generate_tfrecords\\\\valid_00000-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00001-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00002-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00003-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00004-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00005-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00006-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00007-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00008-of-00010',\n",
      " 'generate_tfrecords\\\\valid_00009-of-00010']\n",
      "['generate_tfrecords\\\\test_00000-of-00010',\n",
      " 'generate_tfrecords\\\\test_00001-of-00010',\n",
      " 'generate_tfrecords\\\\test_00002-of-00010',\n",
      " 'generate_tfrecords\\\\test_00003-of-00010',\n",
      " 'generate_tfrecords\\\\test_00004-of-00010',\n",
      " 'generate_tfrecords\\\\test_00005-of-00010',\n",
      " 'generate_tfrecords\\\\test_00006-of-00010',\n",
      " 'generate_tfrecords\\\\test_00007-of-00010',\n",
      " 'generate_tfrecords\\\\test_00008-of-00010',\n",
      " 'generate_tfrecords\\\\test_00009-of-00010']\n"
     ]
    }
   ],
   "source": [
    "# 打印一下文件名\n",
    "pprint.pprint(train_tfrecord_filenames)\n",
    "pprint.pprint(valid_tfrecord_filenames)\n",
    "pprint.pprint(test_tfrecord_fielnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 把数据读取出来\n",
    "expected_features = {\n",
    "    \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
    "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
    "}\n",
    "\n",
    "def parse_example(serialized_example):\n",
    "    example = tf.io.parse_single_example(serialized_example,\n",
    "                                         expected_features)\n",
    "    return example[\"input_features\"], example[\"label\"]\n",
    "\n",
    "def tfrecords_reader_dataset(filenames, n_readers=5,\n",
    "                             batch_size=32, n_parse_threads=5,\n",
    "                             shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()  # 为了能够无限次epoch\n",
    "    dataset = dataset.interleave(\n",
    "#         lambda filename: tf.data.TFRecordDataset(\n",
    "#             filename, compression_type = \"GZIP\"),\n",
    "          lambda filename: tf.data.TFRecordDataset(\n",
    "            filename),\n",
    "          cycle_length = n_readers\n",
    "    )\n",
    "    # 洗牌，就是给数据打乱,样本顺序打乱\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_example,\n",
    "                          num_parallel_calls=n_parse_threads)  # 把对应的一个样本是字节流的，变为浮点类型\n",
    "    dataset = dataset.batch(batch_size)  # 原来写进去是一条一条的sample，要分配\n",
    "    return dataset\n",
    "\n",
    "# 测试一下，tfrecords_reader_dataset是否可以正常运行\n",
    "# tfrecords_train = tfrecords_reader_dataset(train_tfrecord_filenames,\n",
    "#                                            batch_size = 3)\n",
    "# for x_batch, y_batch in tfrecords_train.take(10):\n",
    "#     print(x_batch)\n",
    "#     print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB940> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB940>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB940> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB940>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function parse_example at 0x00000246BE5BB550> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function parse_example at 0x00000246BE5BB550>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function parse_example at 0x00000246BE5BB550> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function parse_example at 0x00000246BE5BB550>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB8B0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB8B0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB8B0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB8B0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB9D0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB9D0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB9D0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x00000246BE5BB9D0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 66 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 得到dataset,dataset是tensor，可以直接拿tensor训练\n",
    "\n",
    "batch_size = 32\n",
    "tfrecords_train_set = tfrecords_reader_dataset(\n",
    "    train_tfrecord_filenames, batch_size = batch_size)\n",
    "tfrecords_valid_set = tfrecords_reader_dataset(\n",
    "    valid_tfrecord_filenames, batch_size = batch_size)\n",
    "tfrecords_test_set = tfrecords_reader_dataset(\n",
    "    test_tfrecord_fielnames, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfrecords_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 8), dtype=float32, numpy=\n",
      "array([[-2.77109686e-02,  1.87416613e+00, -2.01294750e-01,\n",
      "         2.25147326e-02,  1.41042203e-01, -1.04131013e-01,\n",
      "         9.96572435e-01, -1.44064176e+00],\n",
      "       [-3.91372681e-01,  3.18607129e-02, -3.26981932e-01,\n",
      "        -5.25816232e-02, -5.13111115e-01,  3.54368538e-02,\n",
      "         5.95323086e-01, -1.08111203e+00],\n",
      "       [-1.10068619e+00,  1.23336422e+00, -4.57008183e-01,\n",
      "        -2.96145111e-01, -8.38826895e-01,  8.82104039e-02,\n",
      "        -7.20401347e-01,  1.13598776e+00],\n",
      "       [-8.39276016e-01,  1.47366500e+00, -6.63657010e-01,\n",
      "        -2.21059039e-01, -5.04945576e-01,  1.01379789e-01,\n",
      "        -7.53061175e-01,  6.81582153e-01],\n",
      "       [-6.91466868e-01,  6.72662616e-01, -3.54697227e-01,\n",
      "        -1.24215007e-01,  4.97605681e-01,  1.00189252e-02,\n",
      "        -7.29732752e-01,  7.36510277e-01],\n",
      "       [ 9.33741331e-01, -4.48740691e-01,  1.85706854e-01,\n",
      "        -6.31846488e-02, -1.69249669e-01,  1.85529351e-01,\n",
      "        -6.36418939e-01,  5.56745410e-01],\n",
      "       [-1.03509390e+00,  1.92061186e-01, -8.54852438e-01,\n",
      "        -1.50044672e-02,  7.95195520e-01, -1.53023258e-01,\n",
      "         1.01056945e+00, -1.41567433e+00],\n",
      "       [-1.88361526e-01, -8.49241912e-01, -4.84558800e-03,\n",
      "        -1.93672225e-01,  1.15085173e+00, -7.87813887e-02,\n",
      "         1.42115021e+00, -6.36693418e-01],\n",
      "       [ 1.54199809e-01,  1.87416613e+00,  1.12310909e-01,\n",
      "        -5.12976460e-02, -3.61594349e-01, -1.03040874e-01,\n",
      "        -8.46374989e-01,  6.96562529e-01],\n",
      "       [-4.42418545e-01, -1.08954263e+00, -6.37977421e-01,\n",
      "         3.00925821e-02, -3.57965201e-01, -1.77493945e-01,\n",
      "        -1.29428124e+00,  1.17094195e+00],\n",
      "       [-8.12687457e-01, -1.16964281e+00, -4.60726649e-01,\n",
      "         6.34088218e-02, -1.93746388e-01, -1.12179749e-01,\n",
      "         1.92037892e+00, -1.13104677e+00],\n",
      "       [-1.40168619e+00,  9.12963331e-01, -5.87850213e-01,\n",
      "        -1.40846282e-01,  7.17168927e-01,  1.86265126e-01,\n",
      "         5.20672083e-01, -9.73988622e-02],\n",
      "       [-6.78039372e-01,  5.92562377e-01,  7.88635947e-03,\n",
      "        -8.60058069e-02, -1.45660222e-01, -2.98366006e-02,\n",
      "        -1.36893225e+00,  1.25083745e+00],\n",
      "       [-8.86165738e-01,  4.32361901e-01, -3.20695907e-01,\n",
      "        -9.00600329e-02, -7.61707544e-01, -8.38399678e-02,\n",
      "         1.38382471e+00, -9.21321094e-01],\n",
      "       [-1.21521093e-02, -3.68640482e-01, -1.24251381e-01,\n",
      "        -2.58395523e-01, -2.30945125e-01, -9.48837176e-02,\n",
      "         8.93927276e-01, -1.21593571e+00],\n",
      "       [-9.41101313e-01,  5.12462139e-01, -5.07152319e-01,\n",
      "        -8.24168175e-02,  7.20798075e-01,  2.02130541e-04,\n",
      "        -7.57726908e-01,  5.51751971e-01],\n",
      "       [ 7.91100681e-01,  2.72161424e-01,  1.49001420e-01,\n",
      "        -1.30292848e-01, -4.05144066e-01, -7.35770464e-02,\n",
      "        -6.31753266e-01,  1.52274534e-01],\n",
      "       [ 1.16552579e+00, -8.49241912e-01,  4.55393523e-01,\n",
      "         6.40006810e-02,  7.94288278e-01, -9.11743119e-02,\n",
      "         6.13985896e-01, -1.15102065e+00],\n",
      "       [-4.87922877e-01, -2.08439991e-01, -5.58837056e-02,\n",
      "        -1.05132582e-02, -1.24792643e-01, -7.70795271e-02,\n",
      "        -5.78734390e-02, -5.56797922e-01],\n",
      "       [ 7.48953223e-01, -1.00944233e+00,  2.18597949e-01,\n",
      "        -1.45975230e-02,  1.40221436e-02, -2.94891018e-02,\n",
      "        -1.25229001e+00,  1.22587013e+00],\n",
      "       [-1.00935781e+00,  5.92562377e-01, -1.81412414e-01,\n",
      "        -9.09205228e-02, -7.16343224e-01, -5.85738868e-02,\n",
      "         6.88636899e-01, -3.47072244e-01],\n",
      "       [ 9.47754979e-01,  2.72161424e-01,  5.40377378e-01,\n",
      "        -1.90101191e-01, -3.45263183e-01, -2.46532727e-02,\n",
      "         9.68578279e-01, -1.25088990e+00],\n",
      "       [ 6.29597604e-01, -1.97064519e+00,  7.08705664e-01,\n",
      "        -9.53302458e-02,  3.63863039e+00,  1.17618948e-01,\n",
      "        -7.99718082e-01,  1.18092895e+00],\n",
      "       [ 4.57437754e-01,  5.92562377e-01,  4.75749880e-01,\n",
      "         1.11590410e-02, -5.36700547e-01,  1.36301536e-02,\n",
      "        -7.81055331e-01,  7.21529901e-01],\n",
      "       [-6.05520129e-01, -6.08941197e-01,  5.79626895e-02,\n",
      "         3.55128348e-01, -2.92640597e-01, -1.13491543e-01,\n",
      "        -1.13861717e-01, -6.41686857e-01],\n",
      "       [-8.70660186e-01, -3.68640482e-01,  2.13798836e-01,\n",
      "         8.49820599e-02, -7.14528680e-01, -9.54180211e-02,\n",
      "         2.06968117e+00, -6.66654229e-01],\n",
      "       [-2.22729564e-01, -2.08439991e-01, -6.81557894e-01,\n",
      "        -1.05293550e-01, -2.48183563e-01, -5.37950918e-02,\n",
      "        -7.15735674e-01,  6.91569090e-01],\n",
      "       [-4.51476783e-01,  5.12462139e-01, -8.44109178e-01,\n",
      "        -5.05243093e-02, -8.57793391e-02, -2.05928653e-01,\n",
      "        -7.57726908e-01,  5.36771536e-01],\n",
      "       [-7.04841077e-01, -2.08439991e-01, -3.52420956e-02,\n",
      "        -1.20929442e-01,  1.23432207e+00,  6.60271719e-02,\n",
      "        -4.35794294e-01,  7.26523340e-01],\n",
      "       [ 1.67992723e+00, -5.28840959e-01,  6.36577964e-01,\n",
      "        -1.25900298e-01,  2.38203907e+00, -8.35350901e-02,\n",
      "        -7.01738596e-01,  3.76980603e-01],\n",
      "       [ 6.27626121e-01, -9.29342151e-01,  4.20038477e-02,\n",
      "        -2.88925588e-01,  4.31373775e-01,  9.97283384e-02,\n",
      "         7.58622229e-01, -1.13104677e+00],\n",
      "       [-2.95728326e-01, -1.00944233e+00, -1.11609235e-01,\n",
      "        -1.11112274e-01, -4.97687250e-01, -4.71992865e-02,\n",
      "         1.25785100e+00, -1.55549145e+00]], dtype=float32)>, <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[5.00001],\n",
      "       [1.56   ],\n",
      "       [0.634  ],\n",
      "       [1.45   ],\n",
      "       [2.081  ],\n",
      "       [2.176  ],\n",
      "       [5.00001],\n",
      "       [1.414  ],\n",
      "       [2.684  ],\n",
      "       [2.042  ],\n",
      "       [1.023  ],\n",
      "       [0.491  ],\n",
      "       [1.091  ],\n",
      "       [0.844  ],\n",
      "       [2.161  ],\n",
      "       [3.325  ],\n",
      "       [3.852  ],\n",
      "       [3.211  ],\n",
      "       [1.814  ],\n",
      "       [1.563  ],\n",
      "       [0.57   ],\n",
      "       [2.747  ],\n",
      "       [1.526  ],\n",
      "       [2.519  ],\n",
      "       [1.862  ],\n",
      "       [0.696  ],\n",
      "       [1.438  ],\n",
      "       [3.5    ],\n",
      "       [1.04   ],\n",
      "       [3.855  ],\n",
      "       [2.508  ],\n",
      "       [1.602  ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for i in tfrecords_train_set.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 1ms/step - loss: 0.9218 - val_loss: 1.2317\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.7446 - val_loss: 0.9708\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4796 - val_loss: 0.5078\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4417 - val_loss: 0.4749\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4205 - val_loss: 0.4618\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.4468\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.4394\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3960 - val_loss: 0.4305\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3924 - val_loss: 0.4229\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3852 - val_loss: 0.4140\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.4348\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3822 - val_loss: 0.4087\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3722 - val_loss: 0.4069\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3816 - val_loss: 0.4055\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 0.4018\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.3987\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3728 - val_loss: 0.3953\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3609 - val_loss: 0.4052\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3688 - val_loss: 0.3998\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3944\n"
     ]
    }
   ],
   "source": [
    " #开始训练\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu',\n",
    "                       input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "    patience=5, min_delta=1e-2)]\n",
    "\n",
    "history = model.fit(tfrecords_train_set,\n",
    "                    validation_data = tfrecords_valid_set,\n",
    "                    steps_per_epoch = 11160 // batch_size,\n",
    "                    validation_steps = 3870 // batch_size,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 725us/step - loss: 0.3824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3824114501476288"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tfrecords_test_set, steps = 5160 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "sys.version_info(major=3, minor=9, micro=7, releaselevel='final', serial=0)\n",
      "matplotlib 3.5.2\n",
      "numpy 1.23.0\n",
      "pandas 1.4.3\n",
      "sklearn 1.1.1\n",
      "tensorflow 2.9.1\n",
      "keras.api._v2.keras 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.layers.core.dense.Dense'>\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 100), dtype=float32, numpy=\n",
       "array([[ 3.52346838e-01,  3.01123559e-02, -4.12447929e-01,\n",
       "        -3.94739389e-01,  3.64843279e-01, -6.38917506e-01,\n",
       "        -1.19742960e-01,  2.20532984e-01,  2.54925370e-01,\n",
       "         4.19780850e-01,  1.29562691e-01, -2.27048069e-01,\n",
       "         2.09648654e-01, -2.71335363e-01,  2.16068372e-01,\n",
       "         7.22914487e-02, -1.07321993e-01, -2.24011093e-01,\n",
       "        -1.21806502e-01,  1.77026093e-02,  2.17786431e-02,\n",
       "        -2.33111337e-01,  1.55561253e-01, -1.40160292e-01,\n",
       "         3.00422400e-01,  6.12778246e-01, -1.63135558e-01,\n",
       "        -2.49504432e-01,  7.95426220e-02, -3.40925753e-01,\n",
       "         1.46456897e-01, -2.23784700e-01, -5.32576442e-03,\n",
       "         2.92024732e-01,  2.46511176e-01, -9.40905660e-02,\n",
       "        -1.45723268e-01,  8.46904963e-02, -6.45129383e-02,\n",
       "        -4.05768603e-02,  2.91939735e-01,  3.85644019e-01,\n",
       "         3.82549822e-01, -3.37747872e-01,  3.58836085e-01,\n",
       "         1.18517071e-01,  1.92804724e-01,  6.71285093e-02,\n",
       "        -6.76608145e-01,  8.72591585e-02,  1.99830547e-01,\n",
       "        -5.07742882e-01,  5.80554366e-01, -8.31200927e-02,\n",
       "        -6.05565608e-01, -2.83116996e-01,  3.34863961e-01,\n",
       "         6.92934543e-02,  7.41163045e-02, -2.83794671e-01,\n",
       "        -1.57927722e-02, -2.18504131e-01,  5.53106517e-02,\n",
       "        -4.20867205e-01, -2.07338259e-01,  6.91906452e-01,\n",
       "        -5.78907847e-01,  3.18081379e-01,  8.00526440e-02,\n",
       "         1.50785714e-01,  8.83057117e-02,  1.63812578e-01,\n",
       "        -4.25302029e-01,  1.27415657e-02,  3.33746672e-02,\n",
       "        -2.31799513e-01, -2.36965433e-01, -3.02300721e-01,\n",
       "        -2.22706616e-01,  2.27591857e-01, -4.32145596e-01,\n",
       "        -9.30301696e-02,  2.98520267e-01,  3.31040353e-01,\n",
       "        -2.77034491e-02, -2.14073181e-01,  1.78863272e-01,\n",
       "         4.05928373e-01,  3.40555787e-01, -1.19360164e-01,\n",
       "        -1.97644681e-01,  4.03746963e-03,  3.76350999e-01,\n",
       "         1.71875179e-01, -1.17111549e-01, -1.40591189e-01,\n",
       "         6.81385398e-04, -4.15490985e-01, -2.16936454e-01,\n",
       "        -1.74214318e-01],\n",
       "       [ 3.52346838e-01,  3.01123559e-02, -4.12447929e-01,\n",
       "        -3.94739389e-01,  3.64843279e-01, -6.38917506e-01,\n",
       "        -1.19742960e-01,  2.20532984e-01,  2.54925370e-01,\n",
       "         4.19780850e-01,  1.29562691e-01, -2.27048069e-01,\n",
       "         2.09648654e-01, -2.71335363e-01,  2.16068372e-01,\n",
       "         7.22914487e-02, -1.07321993e-01, -2.24011093e-01,\n",
       "        -1.21806502e-01,  1.77026093e-02,  2.17786431e-02,\n",
       "        -2.33111337e-01,  1.55561253e-01, -1.40160292e-01,\n",
       "         3.00422400e-01,  6.12778246e-01, -1.63135558e-01,\n",
       "        -2.49504432e-01,  7.95426220e-02, -3.40925753e-01,\n",
       "         1.46456897e-01, -2.23784700e-01, -5.32576442e-03,\n",
       "         2.92024732e-01,  2.46511176e-01, -9.40905660e-02,\n",
       "        -1.45723268e-01,  8.46904963e-02, -6.45129383e-02,\n",
       "        -4.05768603e-02,  2.91939735e-01,  3.85644019e-01,\n",
       "         3.82549822e-01, -3.37747872e-01,  3.58836085e-01,\n",
       "         1.18517071e-01,  1.92804724e-01,  6.71285093e-02,\n",
       "        -6.76608145e-01,  8.72591585e-02,  1.99830547e-01,\n",
       "        -5.07742882e-01,  5.80554366e-01, -8.31200927e-02,\n",
       "        -6.05565608e-01, -2.83116996e-01,  3.34863961e-01,\n",
       "         6.92934543e-02,  7.41163045e-02, -2.83794671e-01,\n",
       "        -1.57927722e-02, -2.18504131e-01,  5.53106517e-02,\n",
       "        -4.20867205e-01, -2.07338259e-01,  6.91906452e-01,\n",
       "        -5.78907847e-01,  3.18081379e-01,  8.00526440e-02,\n",
       "         1.50785714e-01,  8.83057117e-02,  1.63812578e-01,\n",
       "        -4.25302029e-01,  1.27415657e-02,  3.33746672e-02,\n",
       "        -2.31799513e-01, -2.36965433e-01, -3.02300721e-01,\n",
       "        -2.22706616e-01,  2.27591857e-01, -4.32145596e-01,\n",
       "        -9.30301696e-02,  2.98520267e-01,  3.31040353e-01,\n",
       "        -2.77034491e-02, -2.14073181e-01,  1.78863272e-01,\n",
       "         4.05928373e-01,  3.40555787e-01, -1.19360164e-01,\n",
       "        -1.97644681e-01,  4.03746963e-03,  3.76350999e-01,\n",
       "         1.71875179e-01, -1.17111549e-01, -1.40591189e-01,\n",
       "         6.81385398e-04, -4.15490985e-01, -2.16936454e-01,\n",
       "        -1.74214318e-01],\n",
       "       [ 3.52346838e-01,  3.01123559e-02, -4.12447929e-01,\n",
       "        -3.94739389e-01,  3.64843279e-01, -6.38917506e-01,\n",
       "        -1.19742960e-01,  2.20532984e-01,  2.54925370e-01,\n",
       "         4.19780850e-01,  1.29562691e-01, -2.27048069e-01,\n",
       "         2.09648654e-01, -2.71335363e-01,  2.16068372e-01,\n",
       "         7.22914487e-02, -1.07321993e-01, -2.24011093e-01,\n",
       "        -1.21806502e-01,  1.77026093e-02,  2.17786431e-02,\n",
       "        -2.33111337e-01,  1.55561253e-01, -1.40160292e-01,\n",
       "         3.00422400e-01,  6.12778246e-01, -1.63135558e-01,\n",
       "        -2.49504432e-01,  7.95426220e-02, -3.40925753e-01,\n",
       "         1.46456897e-01, -2.23784700e-01, -5.32576442e-03,\n",
       "         2.92024732e-01,  2.46511176e-01, -9.40905660e-02,\n",
       "        -1.45723268e-01,  8.46904963e-02, -6.45129383e-02,\n",
       "        -4.05768603e-02,  2.91939735e-01,  3.85644019e-01,\n",
       "         3.82549822e-01, -3.37747872e-01,  3.58836085e-01,\n",
       "         1.18517071e-01,  1.92804724e-01,  6.71285093e-02,\n",
       "        -6.76608145e-01,  8.72591585e-02,  1.99830547e-01,\n",
       "        -5.07742882e-01,  5.80554366e-01, -8.31200927e-02,\n",
       "        -6.05565608e-01, -2.83116996e-01,  3.34863961e-01,\n",
       "         6.92934543e-02,  7.41163045e-02, -2.83794671e-01,\n",
       "        -1.57927722e-02, -2.18504131e-01,  5.53106517e-02,\n",
       "        -4.20867205e-01, -2.07338259e-01,  6.91906452e-01,\n",
       "        -5.78907847e-01,  3.18081379e-01,  8.00526440e-02,\n",
       "         1.50785714e-01,  8.83057117e-02,  1.63812578e-01,\n",
       "        -4.25302029e-01,  1.27415657e-02,  3.33746672e-02,\n",
       "        -2.31799513e-01, -2.36965433e-01, -3.02300721e-01,\n",
       "        -2.22706616e-01,  2.27591857e-01, -4.32145596e-01,\n",
       "        -9.30301696e-02,  2.98520267e-01,  3.31040353e-01,\n",
       "        -2.77034491e-02, -2.14073181e-01,  1.78863272e-01,\n",
       "         4.05928373e-01,  3.40555787e-01, -1.19360164e-01,\n",
       "        -1.97644681e-01,  4.03746963e-03,  3.76350999e-01,\n",
       "         1.71875179e-01, -1.17111549e-01, -1.40591189e-01,\n",
       "         6.81385398e-04, -4.15490985e-01, -2.16936454e-01,\n",
       "        -1.74214318e-01],\n",
       "       [ 3.52346838e-01,  3.01123559e-02, -4.12447929e-01,\n",
       "        -3.94739389e-01,  3.64843279e-01, -6.38917506e-01,\n",
       "        -1.19742960e-01,  2.20532984e-01,  2.54925370e-01,\n",
       "         4.19780850e-01,  1.29562691e-01, -2.27048069e-01,\n",
       "         2.09648654e-01, -2.71335363e-01,  2.16068372e-01,\n",
       "         7.22914487e-02, -1.07321993e-01, -2.24011093e-01,\n",
       "        -1.21806502e-01,  1.77026093e-02,  2.17786431e-02,\n",
       "        -2.33111337e-01,  1.55561253e-01, -1.40160292e-01,\n",
       "         3.00422400e-01,  6.12778246e-01, -1.63135558e-01,\n",
       "        -2.49504432e-01,  7.95426220e-02, -3.40925753e-01,\n",
       "         1.46456897e-01, -2.23784700e-01, -5.32576442e-03,\n",
       "         2.92024732e-01,  2.46511176e-01, -9.40905660e-02,\n",
       "        -1.45723268e-01,  8.46904963e-02, -6.45129383e-02,\n",
       "        -4.05768603e-02,  2.91939735e-01,  3.85644019e-01,\n",
       "         3.82549822e-01, -3.37747872e-01,  3.58836085e-01,\n",
       "         1.18517071e-01,  1.92804724e-01,  6.71285093e-02,\n",
       "        -6.76608145e-01,  8.72591585e-02,  1.99830547e-01,\n",
       "        -5.07742882e-01,  5.80554366e-01, -8.31200927e-02,\n",
       "        -6.05565608e-01, -2.83116996e-01,  3.34863961e-01,\n",
       "         6.92934543e-02,  7.41163045e-02, -2.83794671e-01,\n",
       "        -1.57927722e-02, -2.18504131e-01,  5.53106517e-02,\n",
       "        -4.20867205e-01, -2.07338259e-01,  6.91906452e-01,\n",
       "        -5.78907847e-01,  3.18081379e-01,  8.00526440e-02,\n",
       "         1.50785714e-01,  8.83057117e-02,  1.63812578e-01,\n",
       "        -4.25302029e-01,  1.27415657e-02,  3.33746672e-02,\n",
       "        -2.31799513e-01, -2.36965433e-01, -3.02300721e-01,\n",
       "        -2.22706616e-01,  2.27591857e-01, -4.32145596e-01,\n",
       "        -9.30301696e-02,  2.98520267e-01,  3.31040353e-01,\n",
       "        -2.77034491e-02, -2.14073181e-01,  1.78863272e-01,\n",
       "         4.05928373e-01,  3.40555787e-01, -1.19360164e-01,\n",
       "        -1.97644681e-01,  4.03746963e-03,  3.76350999e-01,\n",
       "         1.71875179e-01, -1.17111549e-01, -1.40591189e-01,\n",
       "         6.81385398e-04, -4.15490985e-01, -2.16936454e-01,\n",
       "        -1.74214318e-01],\n",
       "       [ 3.52346838e-01,  3.01123559e-02, -4.12447929e-01,\n",
       "        -3.94739389e-01,  3.64843279e-01, -6.38917506e-01,\n",
       "        -1.19742960e-01,  2.20532984e-01,  2.54925370e-01,\n",
       "         4.19780850e-01,  1.29562691e-01, -2.27048069e-01,\n",
       "         2.09648654e-01, -2.71335363e-01,  2.16068372e-01,\n",
       "         7.22914487e-02, -1.07321993e-01, -2.24011093e-01,\n",
       "        -1.21806502e-01,  1.77026093e-02,  2.17786431e-02,\n",
       "        -2.33111337e-01,  1.55561253e-01, -1.40160292e-01,\n",
       "         3.00422400e-01,  6.12778246e-01, -1.63135558e-01,\n",
       "        -2.49504432e-01,  7.95426220e-02, -3.40925753e-01,\n",
       "         1.46456897e-01, -2.23784700e-01, -5.32576442e-03,\n",
       "         2.92024732e-01,  2.46511176e-01, -9.40905660e-02,\n",
       "        -1.45723268e-01,  8.46904963e-02, -6.45129383e-02,\n",
       "        -4.05768603e-02,  2.91939735e-01,  3.85644019e-01,\n",
       "         3.82549822e-01, -3.37747872e-01,  3.58836085e-01,\n",
       "         1.18517071e-01,  1.92804724e-01,  6.71285093e-02,\n",
       "        -6.76608145e-01,  8.72591585e-02,  1.99830547e-01,\n",
       "        -5.07742882e-01,  5.80554366e-01, -8.31200927e-02,\n",
       "        -6.05565608e-01, -2.83116996e-01,  3.34863961e-01,\n",
       "         6.92934543e-02,  7.41163045e-02, -2.83794671e-01,\n",
       "        -1.57927722e-02, -2.18504131e-01,  5.53106517e-02,\n",
       "        -4.20867205e-01, -2.07338259e-01,  6.91906452e-01,\n",
       "        -5.78907847e-01,  3.18081379e-01,  8.00526440e-02,\n",
       "         1.50785714e-01,  8.83057117e-02,  1.63812578e-01,\n",
       "        -4.25302029e-01,  1.27415657e-02,  3.33746672e-02,\n",
       "        -2.31799513e-01, -2.36965433e-01, -3.02300721e-01,\n",
       "        -2.22706616e-01,  2.27591857e-01, -4.32145596e-01,\n",
       "        -9.30301696e-02,  2.98520267e-01,  3.31040353e-01,\n",
       "        -2.77034491e-02, -2.14073181e-01,  1.78863272e-01,\n",
       "         4.05928373e-01,  3.40555787e-01, -1.19360164e-01,\n",
       "        -1.97644681e-01,  4.03746963e-03,  3.76350999e-01,\n",
       "         1.71875179e-01, -1.17111549e-01, -1.40591189e-01,\n",
       "         6.81385398e-04, -4.15490985e-01, -2.16936454e-01,\n",
       "        -1.74214318e-01],\n",
       "       [ 3.52346838e-01,  3.01123559e-02, -4.12447929e-01,\n",
       "        -3.94739389e-01,  3.64843279e-01, -6.38917506e-01,\n",
       "        -1.19742960e-01,  2.20532984e-01,  2.54925370e-01,\n",
       "         4.19780850e-01,  1.29562691e-01, -2.27048069e-01,\n",
       "         2.09648654e-01, -2.71335363e-01,  2.16068372e-01,\n",
       "         7.22914487e-02, -1.07321993e-01, -2.24011093e-01,\n",
       "        -1.21806502e-01,  1.77026093e-02,  2.17786431e-02,\n",
       "        -2.33111337e-01,  1.55561253e-01, -1.40160292e-01,\n",
       "         3.00422400e-01,  6.12778246e-01, -1.63135558e-01,\n",
       "        -2.49504432e-01,  7.95426220e-02, -3.40925753e-01,\n",
       "         1.46456897e-01, -2.23784700e-01, -5.32576442e-03,\n",
       "         2.92024732e-01,  2.46511176e-01, -9.40905660e-02,\n",
       "        -1.45723268e-01,  8.46904963e-02, -6.45129383e-02,\n",
       "        -4.05768603e-02,  2.91939735e-01,  3.85644019e-01,\n",
       "         3.82549822e-01, -3.37747872e-01,  3.58836085e-01,\n",
       "         1.18517071e-01,  1.92804724e-01,  6.71285093e-02,\n",
       "        -6.76608145e-01,  8.72591585e-02,  1.99830547e-01,\n",
       "        -5.07742882e-01,  5.80554366e-01, -8.31200927e-02,\n",
       "        -6.05565608e-01, -2.83116996e-01,  3.34863961e-01,\n",
       "         6.92934543e-02,  7.41163045e-02, -2.83794671e-01,\n",
       "        -1.57927722e-02, -2.18504131e-01,  5.53106517e-02,\n",
       "        -4.20867205e-01, -2.07338259e-01,  6.91906452e-01,\n",
       "        -5.78907847e-01,  3.18081379e-01,  8.00526440e-02,\n",
       "         1.50785714e-01,  8.83057117e-02,  1.63812578e-01,\n",
       "        -4.25302029e-01,  1.27415657e-02,  3.33746672e-02,\n",
       "        -2.31799513e-01, -2.36965433e-01, -3.02300721e-01,\n",
       "        -2.22706616e-01,  2.27591857e-01, -4.32145596e-01,\n",
       "        -9.30301696e-02,  2.98520267e-01,  3.31040353e-01,\n",
       "        -2.77034491e-02, -2.14073181e-01,  1.78863272e-01,\n",
       "         4.05928373e-01,  3.40555787e-01, -1.19360164e-01,\n",
       "        -1.97644681e-01,  4.03746963e-03,  3.76350999e-01,\n",
       "         1.71875179e-01, -1.17111549e-01, -1.40591189e-01,\n",
       "         6.81385398e-04, -4.15490985e-01, -2.16936454e-01,\n",
       "        -1.74214318e-01],\n",
       "       [ 3.52346838e-01,  3.01123559e-02, -4.12447929e-01,\n",
       "        -3.94739389e-01,  3.64843279e-01, -6.38917506e-01,\n",
       "        -1.19742960e-01,  2.20532984e-01,  2.54925370e-01,\n",
       "         4.19780850e-01,  1.29562691e-01, -2.27048069e-01,\n",
       "         2.09648654e-01, -2.71335363e-01,  2.16068372e-01,\n",
       "         7.22914487e-02, -1.07321993e-01, -2.24011093e-01,\n",
       "        -1.21806502e-01,  1.77026093e-02,  2.17786431e-02,\n",
       "        -2.33111337e-01,  1.55561253e-01, -1.40160292e-01,\n",
       "         3.00422400e-01,  6.12778246e-01, -1.63135558e-01,\n",
       "        -2.49504432e-01,  7.95426220e-02, -3.40925753e-01,\n",
       "         1.46456897e-01, -2.23784700e-01, -5.32576442e-03,\n",
       "         2.92024732e-01,  2.46511176e-01, -9.40905660e-02,\n",
       "        -1.45723268e-01,  8.46904963e-02, -6.45129383e-02,\n",
       "        -4.05768603e-02,  2.91939735e-01,  3.85644019e-01,\n",
       "         3.82549822e-01, -3.37747872e-01,  3.58836085e-01,\n",
       "         1.18517071e-01,  1.92804724e-01,  6.71285093e-02,\n",
       "        -6.76608145e-01,  8.72591585e-02,  1.99830547e-01,\n",
       "        -5.07742882e-01,  5.80554366e-01, -8.31200927e-02,\n",
       "        -6.05565608e-01, -2.83116996e-01,  3.34863961e-01,\n",
       "         6.92934543e-02,  7.41163045e-02, -2.83794671e-01,\n",
       "        -1.57927722e-02, -2.18504131e-01,  5.53106517e-02,\n",
       "        -4.20867205e-01, -2.07338259e-01,  6.91906452e-01,\n",
       "        -5.78907847e-01,  3.18081379e-01,  8.00526440e-02,\n",
       "         1.50785714e-01,  8.83057117e-02,  1.63812578e-01,\n",
       "        -4.25302029e-01,  1.27415657e-02,  3.33746672e-02,\n",
       "        -2.31799513e-01, -2.36965433e-01, -3.02300721e-01,\n",
       "        -2.22706616e-01,  2.27591857e-01, -4.32145596e-01,\n",
       "        -9.30301696e-02,  2.98520267e-01,  3.31040353e-01,\n",
       "        -2.77034491e-02, -2.14073181e-01,  1.78863272e-01,\n",
       "         4.05928373e-01,  3.40555787e-01, -1.19360164e-01,\n",
       "        -1.97644681e-01,  4.03746963e-03,  3.76350999e-01,\n",
       "         1.71875179e-01, -1.17111549e-01, -1.40591189e-01,\n",
       "         6.81385398e-04, -4.15490985e-01, -2.16936454e-01,\n",
       "        -1.74214318e-01],\n",
       "       [ 3.52346838e-01,  3.01123559e-02, -4.12447929e-01,\n",
       "        -3.94739389e-01,  3.64843279e-01, -6.38917506e-01,\n",
       "        -1.19742960e-01,  2.20532984e-01,  2.54925370e-01,\n",
       "         4.19780850e-01,  1.29562691e-01, -2.27048069e-01,\n",
       "         2.09648654e-01, -2.71335363e-01,  2.16068372e-01,\n",
       "         7.22914487e-02, -1.07321993e-01, -2.24011093e-01,\n",
       "        -1.21806502e-01,  1.77026093e-02,  2.17786431e-02,\n",
       "        -2.33111337e-01,  1.55561253e-01, -1.40160292e-01,\n",
       "         3.00422400e-01,  6.12778246e-01, -1.63135558e-01,\n",
       "        -2.49504432e-01,  7.95426220e-02, -3.40925753e-01,\n",
       "         1.46456897e-01, -2.23784700e-01, -5.32576442e-03,\n",
       "         2.92024732e-01,  2.46511176e-01, -9.40905660e-02,\n",
       "        -1.45723268e-01,  8.46904963e-02, -6.45129383e-02,\n",
       "        -4.05768603e-02,  2.91939735e-01,  3.85644019e-01,\n",
       "         3.82549822e-01, -3.37747872e-01,  3.58836085e-01,\n",
       "         1.18517071e-01,  1.92804724e-01,  6.71285093e-02,\n",
       "        -6.76608145e-01,  8.72591585e-02,  1.99830547e-01,\n",
       "        -5.07742882e-01,  5.80554366e-01, -8.31200927e-02,\n",
       "        -6.05565608e-01, -2.83116996e-01,  3.34863961e-01,\n",
       "         6.92934543e-02,  7.41163045e-02, -2.83794671e-01,\n",
       "        -1.57927722e-02, -2.18504131e-01,  5.53106517e-02,\n",
       "        -4.20867205e-01, -2.07338259e-01,  6.91906452e-01,\n",
       "        -5.78907847e-01,  3.18081379e-01,  8.00526440e-02,\n",
       "         1.50785714e-01,  8.83057117e-02,  1.63812578e-01,\n",
       "        -4.25302029e-01,  1.27415657e-02,  3.33746672e-02,\n",
       "        -2.31799513e-01, -2.36965433e-01, -3.02300721e-01,\n",
       "        -2.22706616e-01,  2.27591857e-01, -4.32145596e-01,\n",
       "        -9.30301696e-02,  2.98520267e-01,  3.31040353e-01,\n",
       "        -2.77034491e-02, -2.14073181e-01,  1.78863272e-01,\n",
       "         4.05928373e-01,  3.40555787e-01, -1.19360164e-01,\n",
       "        -1.97644681e-01,  4.03746963e-03,  3.76350999e-01,\n",
       "         1.71875179e-01, -1.17111549e-01, -1.40591189e-01,\n",
       "         6.81385398e-04, -4.15490985e-01, -2.16936454e-01,\n",
       "        -1.74214318e-01],\n",
       "       [ 3.52346838e-01,  3.01123559e-02, -4.12447929e-01,\n",
       "        -3.94739389e-01,  3.64843279e-01, -6.38917506e-01,\n",
       "        -1.19742960e-01,  2.20532984e-01,  2.54925370e-01,\n",
       "         4.19780850e-01,  1.29562691e-01, -2.27048069e-01,\n",
       "         2.09648654e-01, -2.71335363e-01,  2.16068372e-01,\n",
       "         7.22914487e-02, -1.07321993e-01, -2.24011093e-01,\n",
       "        -1.21806502e-01,  1.77026093e-02,  2.17786431e-02,\n",
       "        -2.33111337e-01,  1.55561253e-01, -1.40160292e-01,\n",
       "         3.00422400e-01,  6.12778246e-01, -1.63135558e-01,\n",
       "        -2.49504432e-01,  7.95426220e-02, -3.40925753e-01,\n",
       "         1.46456897e-01, -2.23784700e-01, -5.32576442e-03,\n",
       "         2.92024732e-01,  2.46511176e-01, -9.40905660e-02,\n",
       "        -1.45723268e-01,  8.46904963e-02, -6.45129383e-02,\n",
       "        -4.05768603e-02,  2.91939735e-01,  3.85644019e-01,\n",
       "         3.82549822e-01, -3.37747872e-01,  3.58836085e-01,\n",
       "         1.18517071e-01,  1.92804724e-01,  6.71285093e-02,\n",
       "        -6.76608145e-01,  8.72591585e-02,  1.99830547e-01,\n",
       "        -5.07742882e-01,  5.80554366e-01, -8.31200927e-02,\n",
       "        -6.05565608e-01, -2.83116996e-01,  3.34863961e-01,\n",
       "         6.92934543e-02,  7.41163045e-02, -2.83794671e-01,\n",
       "        -1.57927722e-02, -2.18504131e-01,  5.53106517e-02,\n",
       "        -4.20867205e-01, -2.07338259e-01,  6.91906452e-01,\n",
       "        -5.78907847e-01,  3.18081379e-01,  8.00526440e-02,\n",
       "         1.50785714e-01,  8.83057117e-02,  1.63812578e-01,\n",
       "        -4.25302029e-01,  1.27415657e-02,  3.33746672e-02,\n",
       "        -2.31799513e-01, -2.36965433e-01, -3.02300721e-01,\n",
       "        -2.22706616e-01,  2.27591857e-01, -4.32145596e-01,\n",
       "        -9.30301696e-02,  2.98520267e-01,  3.31040353e-01,\n",
       "        -2.77034491e-02, -2.14073181e-01,  1.78863272e-01,\n",
       "         4.05928373e-01,  3.40555787e-01, -1.19360164e-01,\n",
       "        -1.97644681e-01,  4.03746963e-03,  3.76350999e-01,\n",
       "         1.71875179e-01, -1.17111549e-01, -1.40591189e-01,\n",
       "         6.81385398e-04, -4.15490985e-01, -2.16936454e-01,\n",
       "        -1.74214318e-01],\n",
       "       [ 3.52346838e-01,  3.01123559e-02, -4.12447929e-01,\n",
       "        -3.94739389e-01,  3.64843279e-01, -6.38917506e-01,\n",
       "        -1.19742960e-01,  2.20532984e-01,  2.54925370e-01,\n",
       "         4.19780850e-01,  1.29562691e-01, -2.27048069e-01,\n",
       "         2.09648654e-01, -2.71335363e-01,  2.16068372e-01,\n",
       "         7.22914487e-02, -1.07321993e-01, -2.24011093e-01,\n",
       "        -1.21806502e-01,  1.77026093e-02,  2.17786431e-02,\n",
       "        -2.33111337e-01,  1.55561253e-01, -1.40160292e-01,\n",
       "         3.00422400e-01,  6.12778246e-01, -1.63135558e-01,\n",
       "        -2.49504432e-01,  7.95426220e-02, -3.40925753e-01,\n",
       "         1.46456897e-01, -2.23784700e-01, -5.32576442e-03,\n",
       "         2.92024732e-01,  2.46511176e-01, -9.40905660e-02,\n",
       "        -1.45723268e-01,  8.46904963e-02, -6.45129383e-02,\n",
       "        -4.05768603e-02,  2.91939735e-01,  3.85644019e-01,\n",
       "         3.82549822e-01, -3.37747872e-01,  3.58836085e-01,\n",
       "         1.18517071e-01,  1.92804724e-01,  6.71285093e-02,\n",
       "        -6.76608145e-01,  8.72591585e-02,  1.99830547e-01,\n",
       "        -5.07742882e-01,  5.80554366e-01, -8.31200927e-02,\n",
       "        -6.05565608e-01, -2.83116996e-01,  3.34863961e-01,\n",
       "         6.92934543e-02,  7.41163045e-02, -2.83794671e-01,\n",
       "        -1.57927722e-02, -2.18504131e-01,  5.53106517e-02,\n",
       "        -4.20867205e-01, -2.07338259e-01,  6.91906452e-01,\n",
       "        -5.78907847e-01,  3.18081379e-01,  8.00526440e-02,\n",
       "         1.50785714e-01,  8.83057117e-02,  1.63812578e-01,\n",
       "        -4.25302029e-01,  1.27415657e-02,  3.33746672e-02,\n",
       "        -2.31799513e-01, -2.36965433e-01, -3.02300721e-01,\n",
       "        -2.22706616e-01,  2.27591857e-01, -4.32145596e-01,\n",
       "        -9.30301696e-02,  2.98520267e-01,  3.31040353e-01,\n",
       "        -2.77034491e-02, -2.14073181e-01,  1.78863272e-01,\n",
       "         4.05928373e-01,  3.40555787e-01, -1.19360164e-01,\n",
       "        -1.97644681e-01,  4.03746963e-03,  3.76350999e-01,\n",
       "         1.71875179e-01, -1.17111549e-01, -1.40591189e-01,\n",
       "         6.81385398e-04, -4.15490985e-01, -2.16936454e-01,\n",
       "        -1.74214318e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer = tf.keras.layers.Dense(100)\n",
    "layer = tf.keras.layers.Dense(100, input_shape=(None, 5))  # input_shape往往第一层指定\n",
    "print(type(layer))\n",
    "print('-'*50)\n",
    "# layer是输入为5，输出为100的全连接层，所以对于输入为10x5的矩阵来看，会乘以一个5x100的矩阵，所以输出就是10x100\n",
    "layer(tf.ones([10, 5]))  # 这里是对应层的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
      "array([[ 0.17918234, -0.07707007, -0.16100825, -0.12076918,  0.17583461,\n",
      "         0.03338574, -0.0356402 ,  0.10399   ,  0.02757962,  0.10231619,\n",
      "         0.03995182, -0.21766073,  0.1387655 , -0.0073114 ,  0.01430272,\n",
      "         0.1942674 ,  0.00468551, -0.18477494,  0.0991639 , -0.22225276,\n",
      "         0.06872471,  0.03350769, -0.09983937, -0.15619999,  0.12593167,\n",
      "        -0.05684003, -0.14740558,  0.17700161,  0.22240104, -0.21916819,\n",
      "        -0.20806476, -0.14517927, -0.03059125,  0.23109938, -0.02959524,\n",
      "        -0.13148044,  0.07114775, -0.07772145, -0.16225491,  0.02237229,\n",
      "         0.13856904,  0.08158226,  0.16014902, -0.06497315,  0.02871732,\n",
      "         0.13423486,  0.08206002,  0.13501109, -0.20549569, -0.06121498,\n",
      "         0.09362839,  0.07332952,  0.1919197 ,  0.0299805 , -0.12142061,\n",
      "        -0.08617578,  0.14440979, -0.016488  , -0.22776483, -0.02901152,\n",
      "        -0.15356492, -0.1931158 , -0.19000068,  0.02891321, -0.07137972,\n",
      "         0.1461695 , -0.11405217,  0.06983675, -0.22920646,  0.2301599 ,\n",
      "        -0.1144041 ,  0.23386033, -0.21255888, -0.09025449,  0.21880446,\n",
      "         0.06135775, -0.12883979,  0.20336552, -0.09353043, -0.07509373,\n",
      "        -0.22400706,  0.01257093, -0.0932433 , -0.04088669, -0.19231766,\n",
      "        -0.20969738,  0.11173214,  0.16492482,  0.11961462,  0.03884201,\n",
      "        -0.16537721, -0.22850345, -0.03566378,  0.17701648, -0.04608859,\n",
      "         0.17927311,  0.19338526, -0.18390414, -0.09462014,  0.22369696],\n",
      "       [-0.02299733, -0.04453155, -0.02240245, -0.19185244, -0.12357044,\n",
      "        -0.20449871, -0.0683554 ,  0.02348308,  0.0356137 ,  0.171472  ,\n",
      "        -0.17696154, -0.04127321,  0.0243348 , -0.05986641, -0.22777776,\n",
      "        -0.20652664, -0.06650074,  0.15108077, -0.17699996,  0.00546448,\n",
      "         0.14438449,  0.11006637, -0.1793911 , -0.11009002, -0.00615701,\n",
      "         0.17231564,  0.16551419, -0.20259714,  0.22309665,  0.02061941,\n",
      "         0.21032937,  0.06485374, -0.11643111,  0.1831503 , -0.070904  ,\n",
      "         0.16867815, -0.1732789 , -0.14480197,  0.09361883, -0.03947884,\n",
      "         0.11425568,  0.17334248,  0.12460138, -0.19108707,  0.17779781,\n",
      "        -0.02764203, -0.03591552, -0.16621518, -0.18434675, -0.08871567,\n",
      "         0.04108195, -0.18805437,  0.04878135,  0.13184793, -0.16659185,\n",
      "        -0.09515804, -0.00470425,  0.08497648,  0.12725915, -0.13905434,\n",
      "         0.1675653 ,  0.13443397, -0.21438505, -0.14154232,  0.03398101,\n",
      "        -0.00171891, -0.14976001,  0.20680197,  0.01638354, -0.21231997,\n",
      "         0.05495949, -0.08178659, -0.23335874,  0.22884502, -0.16379088,\n",
      "         0.05755399, -0.11318986, -0.13390464, -0.14418098,  0.09675436,\n",
      "        -0.17571712, -0.18542477,  0.21405609, -0.07523838,  0.07183756,\n",
      "        -0.00882855, -0.09352228,  0.22340505, -0.03739586, -0.1906317 ,\n",
      "         0.0475788 ,  0.14586397, -0.11967789, -0.14256015, -0.17840883,\n",
      "        -0.13635635, -0.19790149, -0.11566017,  0.17822827, -0.1415683 ],\n",
      "       [ 0.22567563,  0.11709405, -0.16959554,  0.05520986,  0.15242158,\n",
      "        -0.13842258,  0.19386537,  0.16915311,  0.01699369,  0.2156711 ,\n",
      "         0.1853023 ,  0.12562619, -0.0260423 ,  0.00794977,  0.13477804,\n",
      "         0.20629041, -0.15945189, -0.23346166, -0.10557851, -0.0356952 ,\n",
      "        -0.19504392, -0.1091273 ,  0.21806513,  0.1498972 ,  0.22074445,\n",
      "         0.1424432 ,  0.00247498, -0.18941838, -0.1528704 , -0.09048621,\n",
      "         0.03241251, -0.23557885,  0.15019606, -0.00184156, -0.10593073,\n",
      "        -0.13278353, -0.01476847,  0.02908726,  0.03690036, -0.11074726,\n",
      "        -0.20404139, -0.11337782, -0.16647017, -0.08386266, -0.11686602,\n",
      "         0.05578361, -0.07666309,  0.17737941, -0.19711971,  0.01335754,\n",
      "        -0.01926413, -0.07760023,  0.21697162, -0.16033602, -0.22624278,\n",
      "        -0.12376787, -0.12811062, -0.02419287, -0.06502302, -0.20937109,\n",
      "         0.10362332, -0.1424807 ,  0.17203896, -0.09038226, -0.11631729,\n",
      "         0.21283929, -0.06281956,  0.19341438,  0.11652042,  0.185169  ,\n",
      "        -0.00564782, -0.05934082,  0.15841223,  0.05297704, -0.03406714,\n",
      "        -0.206916  , -0.16394025, -0.20355526, -0.0591222 ,  0.18162407,\n",
      "        -0.03058594, -0.14778373,  0.22978564,  0.18987648, -0.00436155,\n",
      "        -0.09201567,  0.02295904, -0.04657486,  0.20264788,  0.01243152,\n",
      "         0.20765047, -0.21729198,  0.08989428,  0.16962717, -0.15379664,\n",
      "         0.04512848,  0.04654948, -0.12680063, -0.23075782, -0.03496164],\n",
      "       [-0.16440356,  0.01579498, -0.1280465 , -0.10817084, -0.05007993,\n",
      "        -0.15619355, -0.01057561, -0.03529403,  0.09096639, -0.17488945,\n",
      "         0.08208968, -0.05108915,  0.12623633, -0.01987594,  0.14663936,\n",
      "         0.02290533,  0.06603937,  0.06814964, -0.03405175,  0.2198473 ,\n",
      "         0.17812403, -0.12348308,  0.05572034, -0.02052197,  0.14493255,\n",
      "         0.16124956, -0.22241455,  0.10911112, -0.17536399,  0.09398083,\n",
      "         0.23866056, -0.07038036,  0.13923593, -0.19891027,  0.21834286,\n",
      "        -0.06847349, -0.00297822,  0.0606295 , -0.03537939,  0.13971211,\n",
      "         0.14789782,  0.23723756,  0.04610358, -0.05024652,  0.057548  ,\n",
      "        -0.20681337,  0.05766825, -0.06996112,  0.10437973,  0.02318628,\n",
      "        -0.07076716, -0.08849145,  0.10091142,  0.14825313, -0.13809697,\n",
      "        -0.17628983,  0.16087301, -0.15759751,  0.09682582,  0.20952247,\n",
      "        -0.16643079,  0.1139902 ,  0.05323271, -0.22191484,  0.18530793,\n",
      "         0.12388082, -0.15055443, -0.2369868 ,  0.16611846, -0.0113347 ,\n",
      "         0.22843374, -0.11447226,  0.05114944, -0.15066631,  0.05072002,\n",
      "         0.09307648,  0.02105303,  0.05530982,  0.14038388, -0.02427842,\n",
      "         0.04151641,  0.2241721 , -0.20135476,  0.14974843, -0.11451313,\n",
      "         0.20269193, -0.07945448,  0.10198818, -0.10811305, -0.14898588,\n",
      "        -0.22503783,  0.19926293,  0.23657309,  0.16406228,  0.03119259,\n",
      "        -0.14377058, -0.13006833,  0.03217913,  0.05691577, -0.17945328],\n",
      "       [ 0.13488977,  0.01882495,  0.06860481, -0.02915679,  0.21023746,\n",
      "        -0.17318839, -0.19903713, -0.0407992 ,  0.08377199,  0.10521103,\n",
      "        -0.00081956, -0.04265118, -0.05364569, -0.19223139,  0.14812602,\n",
      "        -0.14464507,  0.04790576, -0.02500491,  0.09565981,  0.05033879,\n",
      "        -0.17441066, -0.14407502,  0.16100626, -0.00324552, -0.18502924,\n",
      "         0.19360988,  0.03869541, -0.14360164, -0.03772067, -0.14587161,\n",
      "        -0.12688076,  0.16250004, -0.14773539,  0.07852687,  0.2345983 ,\n",
      "         0.06996875, -0.02584544,  0.21749716,  0.00260219, -0.05243516,\n",
      "         0.09525858,  0.00685956,  0.21816601,  0.05242153,  0.21163897,\n",
      "         0.16295399,  0.16565506, -0.0090857 , -0.1940257 ,  0.200646  ,\n",
      "         0.1551515 , -0.22692634,  0.02197026, -0.23286563,  0.04678665,\n",
      "         0.19827451,  0.16239603,  0.18259536,  0.14281918, -0.11588021,\n",
      "         0.03301431, -0.1313318 ,  0.23442473,  0.00405903, -0.2389302 ,\n",
      "         0.21073572, -0.10172169,  0.0850151 ,  0.01023668, -0.0408885 ,\n",
      "        -0.0750356 ,  0.18555193, -0.18894608, -0.02815969, -0.0382918 ,\n",
      "        -0.23687173,  0.14795147, -0.22351617, -0.06625687,  0.04858558,\n",
      "        -0.0433519 ,  0.00343531,  0.1492766 ,  0.1075405 ,  0.21165134,\n",
      "        -0.10622351,  0.21714886, -0.03781481,  0.16380222,  0.16898389,\n",
      "        -0.0624589 ,  0.104706  ,  0.2052253 , -0.19627059,  0.2299899 ,\n",
      "        -0.08486585,  0.08871646, -0.02130522, -0.12670255, -0.04192805]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(100,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[ 0.17918234, -0.07707007, -0.16100825, -0.12076918,  0.17583461,\n",
       "          0.03338574, -0.0356402 ,  0.10399   ,  0.02757962,  0.10231619,\n",
       "          0.03995182, -0.21766073,  0.1387655 , -0.0073114 ,  0.01430272,\n",
       "          0.1942674 ,  0.00468551, -0.18477494,  0.0991639 , -0.22225276,\n",
       "          0.06872471,  0.03350769, -0.09983937, -0.15619999,  0.12593167,\n",
       "         -0.05684003, -0.14740558,  0.17700161,  0.22240104, -0.21916819,\n",
       "         -0.20806476, -0.14517927, -0.03059125,  0.23109938, -0.02959524,\n",
       "         -0.13148044,  0.07114775, -0.07772145, -0.16225491,  0.02237229,\n",
       "          0.13856904,  0.08158226,  0.16014902, -0.06497315,  0.02871732,\n",
       "          0.13423486,  0.08206002,  0.13501109, -0.20549569, -0.06121498,\n",
       "          0.09362839,  0.07332952,  0.1919197 ,  0.0299805 , -0.12142061,\n",
       "         -0.08617578,  0.14440979, -0.016488  , -0.22776483, -0.02901152,\n",
       "         -0.15356492, -0.1931158 , -0.19000068,  0.02891321, -0.07137972,\n",
       "          0.1461695 , -0.11405217,  0.06983675, -0.22920646,  0.2301599 ,\n",
       "         -0.1144041 ,  0.23386033, -0.21255888, -0.09025449,  0.21880446,\n",
       "          0.06135775, -0.12883979,  0.20336552, -0.09353043, -0.07509373,\n",
       "         -0.22400706,  0.01257093, -0.0932433 , -0.04088669, -0.19231766,\n",
       "         -0.20969738,  0.11173214,  0.16492482,  0.11961462,  0.03884201,\n",
       "         -0.16537721, -0.22850345, -0.03566378,  0.17701648, -0.04608859,\n",
       "          0.17927311,  0.19338526, -0.18390414, -0.09462014,  0.22369696],\n",
       "        [-0.02299733, -0.04453155, -0.02240245, -0.19185244, -0.12357044,\n",
       "         -0.20449871, -0.0683554 ,  0.02348308,  0.0356137 ,  0.171472  ,\n",
       "         -0.17696154, -0.04127321,  0.0243348 , -0.05986641, -0.22777776,\n",
       "         -0.20652664, -0.06650074,  0.15108077, -0.17699996,  0.00546448,\n",
       "          0.14438449,  0.11006637, -0.1793911 , -0.11009002, -0.00615701,\n",
       "          0.17231564,  0.16551419, -0.20259714,  0.22309665,  0.02061941,\n",
       "          0.21032937,  0.06485374, -0.11643111,  0.1831503 , -0.070904  ,\n",
       "          0.16867815, -0.1732789 , -0.14480197,  0.09361883, -0.03947884,\n",
       "          0.11425568,  0.17334248,  0.12460138, -0.19108707,  0.17779781,\n",
       "         -0.02764203, -0.03591552, -0.16621518, -0.18434675, -0.08871567,\n",
       "          0.04108195, -0.18805437,  0.04878135,  0.13184793, -0.16659185,\n",
       "         -0.09515804, -0.00470425,  0.08497648,  0.12725915, -0.13905434,\n",
       "          0.1675653 ,  0.13443397, -0.21438505, -0.14154232,  0.03398101,\n",
       "         -0.00171891, -0.14976001,  0.20680197,  0.01638354, -0.21231997,\n",
       "          0.05495949, -0.08178659, -0.23335874,  0.22884502, -0.16379088,\n",
       "          0.05755399, -0.11318986, -0.13390464, -0.14418098,  0.09675436,\n",
       "         -0.17571712, -0.18542477,  0.21405609, -0.07523838,  0.07183756,\n",
       "         -0.00882855, -0.09352228,  0.22340505, -0.03739586, -0.1906317 ,\n",
       "          0.0475788 ,  0.14586397, -0.11967789, -0.14256015, -0.17840883,\n",
       "         -0.13635635, -0.19790149, -0.11566017,  0.17822827, -0.1415683 ],\n",
       "        [ 0.22567563,  0.11709405, -0.16959554,  0.05520986,  0.15242158,\n",
       "         -0.13842258,  0.19386537,  0.16915311,  0.01699369,  0.2156711 ,\n",
       "          0.1853023 ,  0.12562619, -0.0260423 ,  0.00794977,  0.13477804,\n",
       "          0.20629041, -0.15945189, -0.23346166, -0.10557851, -0.0356952 ,\n",
       "         -0.19504392, -0.1091273 ,  0.21806513,  0.1498972 ,  0.22074445,\n",
       "          0.1424432 ,  0.00247498, -0.18941838, -0.1528704 , -0.09048621,\n",
       "          0.03241251, -0.23557885,  0.15019606, -0.00184156, -0.10593073,\n",
       "         -0.13278353, -0.01476847,  0.02908726,  0.03690036, -0.11074726,\n",
       "         -0.20404139, -0.11337782, -0.16647017, -0.08386266, -0.11686602,\n",
       "          0.05578361, -0.07666309,  0.17737941, -0.19711971,  0.01335754,\n",
       "         -0.01926413, -0.07760023,  0.21697162, -0.16033602, -0.22624278,\n",
       "         -0.12376787, -0.12811062, -0.02419287, -0.06502302, -0.20937109,\n",
       "          0.10362332, -0.1424807 ,  0.17203896, -0.09038226, -0.11631729,\n",
       "          0.21283929, -0.06281956,  0.19341438,  0.11652042,  0.185169  ,\n",
       "         -0.00564782, -0.05934082,  0.15841223,  0.05297704, -0.03406714,\n",
       "         -0.206916  , -0.16394025, -0.20355526, -0.0591222 ,  0.18162407,\n",
       "         -0.03058594, -0.14778373,  0.22978564,  0.18987648, -0.00436155,\n",
       "         -0.09201567,  0.02295904, -0.04657486,  0.20264788,  0.01243152,\n",
       "          0.20765047, -0.21729198,  0.08989428,  0.16962717, -0.15379664,\n",
       "          0.04512848,  0.04654948, -0.12680063, -0.23075782, -0.03496164],\n",
       "        [-0.16440356,  0.01579498, -0.1280465 , -0.10817084, -0.05007993,\n",
       "         -0.15619355, -0.01057561, -0.03529403,  0.09096639, -0.17488945,\n",
       "          0.08208968, -0.05108915,  0.12623633, -0.01987594,  0.14663936,\n",
       "          0.02290533,  0.06603937,  0.06814964, -0.03405175,  0.2198473 ,\n",
       "          0.17812403, -0.12348308,  0.05572034, -0.02052197,  0.14493255,\n",
       "          0.16124956, -0.22241455,  0.10911112, -0.17536399,  0.09398083,\n",
       "          0.23866056, -0.07038036,  0.13923593, -0.19891027,  0.21834286,\n",
       "         -0.06847349, -0.00297822,  0.0606295 , -0.03537939,  0.13971211,\n",
       "          0.14789782,  0.23723756,  0.04610358, -0.05024652,  0.057548  ,\n",
       "         -0.20681337,  0.05766825, -0.06996112,  0.10437973,  0.02318628,\n",
       "         -0.07076716, -0.08849145,  0.10091142,  0.14825313, -0.13809697,\n",
       "         -0.17628983,  0.16087301, -0.15759751,  0.09682582,  0.20952247,\n",
       "         -0.16643079,  0.1139902 ,  0.05323271, -0.22191484,  0.18530793,\n",
       "          0.12388082, -0.15055443, -0.2369868 ,  0.16611846, -0.0113347 ,\n",
       "          0.22843374, -0.11447226,  0.05114944, -0.15066631,  0.05072002,\n",
       "          0.09307648,  0.02105303,  0.05530982,  0.14038388, -0.02427842,\n",
       "          0.04151641,  0.2241721 , -0.20135476,  0.14974843, -0.11451313,\n",
       "          0.20269193, -0.07945448,  0.10198818, -0.10811305, -0.14898588,\n",
       "         -0.22503783,  0.19926293,  0.23657309,  0.16406228,  0.03119259,\n",
       "         -0.14377058, -0.13006833,  0.03217913,  0.05691577, -0.17945328],\n",
       "        [ 0.13488977,  0.01882495,  0.06860481, -0.02915679,  0.21023746,\n",
       "         -0.17318839, -0.19903713, -0.0407992 ,  0.08377199,  0.10521103,\n",
       "         -0.00081956, -0.04265118, -0.05364569, -0.19223139,  0.14812602,\n",
       "         -0.14464507,  0.04790576, -0.02500491,  0.09565981,  0.05033879,\n",
       "         -0.17441066, -0.14407502,  0.16100626, -0.00324552, -0.18502924,\n",
       "          0.19360988,  0.03869541, -0.14360164, -0.03772067, -0.14587161,\n",
       "         -0.12688076,  0.16250004, -0.14773539,  0.07852687,  0.2345983 ,\n",
       "          0.06996875, -0.02584544,  0.21749716,  0.00260219, -0.05243516,\n",
       "          0.09525858,  0.00685956,  0.21816601,  0.05242153,  0.21163897,\n",
       "          0.16295399,  0.16565506, -0.0090857 , -0.1940257 ,  0.200646  ,\n",
       "          0.1551515 , -0.22692634,  0.02197026, -0.23286563,  0.04678665,\n",
       "          0.19827451,  0.16239603,  0.18259536,  0.14281918, -0.11588021,\n",
       "          0.03301431, -0.1313318 ,  0.23442473,  0.00405903, -0.2389302 ,\n",
       "          0.21073572, -0.10172169,  0.0850151 ,  0.01023668, -0.0408885 ,\n",
       "         -0.0750356 ,  0.18555193, -0.18894608, -0.02815969, -0.0382918 ,\n",
       "         -0.23687173,  0.14795147, -0.22351617, -0.06625687,  0.04858558,\n",
       "         -0.0433519 ,  0.00343531,  0.1492766 ,  0.1075405 ,  0.21165134,\n",
       "         -0.10622351,  0.21714886, -0.03781481,  0.16380222,  0.16898389,\n",
       "         -0.0624589 ,  0.104706  ,  0.2052253 , -0.19627059,  0.2299899 ,\n",
       "         -0.08486585,  0.08871646, -0.02130522, -0.12670255, -0.04192805]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer.variables 可以打印layer里包含的所有参数\n",
    "# x * w + b, w就是指层的参数，kernel就是w，bias就是b\n",
    "print(layer.variables)\n",
    "print('-'*50)\n",
    "# 获得所有可训练的变量,和上面的变量数目一致\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module keras.layers.core.dense object:\n",
      "\n",
      "class Dense(keras.engine.base_layer.Layer)\n",
      " |  Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`). These are all attributes of\n",
      " |  `Dense`.\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`,\n",
      " |  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
      " |  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
      " |  (there are `batch_size * d0` such sub-tensors).\n",
      " |  The output in this case will have shape `(batch_size, d0, units)`.\n",
      " |  \n",
      " |  Besides, layer attributes cannot be modified after the layer has been called\n",
      " |  once (except the `trainable` attribute).\n",
      " |  When a popular kwarg `input_shape` is passed, then keras will create\n",
      " |  an input layer to insert before the current layer. This can be treated\n",
      " |  equivalent to explicitly defining an `InputLayer`.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> # Create a `Sequential` model and add a Dense layer as the first layer.\n",
      " |  >>> model = tf.keras.models.Sequential()\n",
      " |  >>> model.add(tf.keras.Input(shape=(16,)))\n",
      " |  >>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
      " |  >>> # Now the model will take as input arrays of shape (None, 16)\n",
      " |  >>> # and output arrays of shape (None, 32).\n",
      " |  >>> # Note that after the first layer, you don't need to specify\n",
      " |  >>> # the size of the input anymore:\n",
      " |  >>> model.add(tf.keras.layers.Dense(32))\n",
      " |  >>> model.output_shape\n",
      " |  (None, 32)\n",
      " |  \n",
      " |  Args:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\").\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call. It is invoked automatically before\n",
      " |      the first execution of `call()`.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses\n",
      " |      (at the discretion of the subclass implementer).\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      The `call()` method may not create state (except in its first invocation,\n",
      " |      wrapping the creation of variables or other resources in `tf.init_scope()`).\n",
      " |      It is recommended to create state in `__init__()`, or the `build()` method\n",
      " |      that is called automatically before `call()` executes the first time.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
      " |          The first positional `inputs` argument is subject to special rules:\n",
      " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
      " |            arguments, and `inputs` cannot be provided via the default value\n",
      " |            of a keyword argument.\n",
      " |          - NumPy array or Python scalar values in `inputs` get cast as tensors.\n",
      " |          - Keras mask metadata is only collected from `inputs`.\n",
      " |          - Layers are built (`build(input_shape)` method)\n",
      " |            using shape info from `inputs` only.\n",
      " |          - `input_spec` compatibility is only checked against `inputs`.\n",
      " |          - Mixed precision input casting is only applied to `inputs`.\n",
      " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
      " |            casting behavior in mixed precision should be handled manually.\n",
      " |          - The SavedModel input specification is generated using `inputs` only.\n",
      " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
      " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
      " |            positional and keyword arguments.\n",
      " |        *args: Additional positional arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |          The following optional keyword arguments are reserved:\n",
      " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
      " |            `mask` argument, its default value will be set to the mask generated\n",
      " |            for `inputs` by the previous layer (if `input` did come from a layer\n",
      " |            that generated a corresponding mask, i.e. if it came from a Keras\n",
      " |            layer with masking support).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      This method will cause the layer's state to be built, if that has not\n",
      " |      happened before. This requires that the layer will later be used with\n",
      " |      inputs that match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of dict\n",
      " |      every time it is called. The callers should make a copy of the returned dict\n",
      " |      if they want to modify it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Used for backwards compatibility only.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after updating\n",
      " |      a layer weights. It can be overridden to finalize any additional layer state\n",
      " |      after a weight update.\n",
      " |      \n",
      " |      This function will be called after weights of a layer have been restored\n",
      " |      from a loaded model.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of NumPy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
      " |      computations and the output to be in the compute dtype as well. This is done\n",
      " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
      " |      these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision when\n",
      " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
      " |      will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Return Functional API nodes upstream of this layer.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Return Functional API nodes downstream of this layer.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "# print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 11)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4.5417706e-05 6.7153489e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01]\n",
      " [4.5417706e-05 6.7153489e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01]], shape=(2, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 使用lamba，一行代码就搞定\n",
    "# tf.math.softplus : log(1+e^x)\n",
    "customized_softplus = keras.layers.Lambda(lambda x : tf.math.log(1+tf.math.exp(x)))\n",
    "print(customized_softplus([[-10., -5., 0., 5., 10.],[-10., -5., 0., 5., 10.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "(None, 8)\n",
      "--------------------------------------------------\n",
      "(None, 30)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " customized_dense_layer_2 (C  (None, 30)               270       \n",
      " ustomizedDenseLayer)                                            \n",
      "                                                                 \n",
      " customized_dense_layer_3 (C  (None, 1)                31        \n",
      " ustomizedDenseLayer)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# customized dense layer\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units  # 神经元个数\n",
    "        self.activation = keras.layers.Activation(activation)  # 激活函数，直接使用tf提供的\n",
    "        super(CustomizedDenseLayer, self).__init__(**kwargs)  # 继承父类\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"构建所需要的参数，也就是kernel还有bias\"\"\"\n",
    "        # x * w + b， input_shape:[None, a]， w:[a,b]， output_shape: [None, b]\n",
    "        print('-'*50)\n",
    "        print(input_shape)\n",
    "        # add_weight是父类里自带的，用来初始化权重\n",
    "        self.kernel = self.add_weight(name = 'kernel',\n",
    "                                      shape = (input_shape[1], self.units),\n",
    "                                      initializer = 'uniform',  # 使用均匀分布的方法去初始化kernel\n",
    "                                      trainable = True)  # 设置为可训练\n",
    "        self.bias = self.add_weight(name = 'bias',\n",
    "                                    shape = (self.units, ),\n",
    "                                    initializer = 'zeros',\n",
    "                                    trainable = True)\n",
    "        # 继承父类的build\n",
    "        super(CustomizedDenseLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"完成正向计算，实现activation（wx+b）\"\"\"\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "\n",
    "# 完全模仿dense来实现自定义层，因此input_shape传的和dense一致，只需要是特征数，父类Layer自动会转为二维的input_shape，然后再传递给build\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation='relu',\n",
    "                         input_shape=x_train.shape[1:]),  # 这里传入的是特征数\n",
    "    CustomizedDenseLayer(1,activation=customized_softplus),\n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss=\"mean_squared_error\", \n",
    "              optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 1.2201 - val_loss: 0.6810\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5926 - val_loss: 0.5878\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5195 - val_loss: 0.5268\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4760 - val_loss: 0.4847\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4524 - val_loss: 0.4642\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4418 - val_loss: 0.4796\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4394 - val_loss: 0.4520\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4195 - val_loss: 0.4319\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.4217\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4326\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train,\n",
    "                    validation_data = (x_valid_scaled, y_valid),\n",
    "                    epochs = 10,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4298405051231384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
